{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcffd6b-c7d2-44f1-9680-3647370391ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb840bc-bb97-4485-9a20-9bd07df3ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "try:\n",
    "    from tqdm.notebook import tqdm  # Jupyter\n",
    "except ImportError:\n",
    "    from tqdm import tqdm  # terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9680c-5350-43ce-827a-c7af4e84560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_AVAILABLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee57d3-6d3e-4fde-bf17-a61c9903dfbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (Function) Data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b521d-0cec-44ac-872c-3eada0a644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariantDataLoader:\n",
    "    \"\"\"\n",
    "    Annovar ì£¼ì„ íŒŒì¼ + STRING PPI ë„¤íŠ¸ì›Œí¬ ë¡œë”\n",
    "    \"\"\"\n",
    "    def __init__(self, ppi_source='string'):\n",
    "        self.ppi_source = ppi_source\n",
    "        self.gene_to_idx = {}  # ìœ ì „ìëª… â†’ ë…¸ë“œ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "        self.idx_to_gene = {}  # ë…¸ë“œ ì¸ë±ìŠ¤ â†’ ìœ ì „ìëª… ë§¤í•‘\n",
    "        self.protein_to_gene = {}  # Protein ID â†’ Gene Symbol ë§¤í•‘\n",
    "        self.gene_to_protein = {}  # Gene Symbol â†’ Protein ID ë§¤í•‘\n",
    "        \n",
    "    def load_annovar_data(self, annovar_file, label_column='Label'):\n",
    "        \"\"\"\n",
    "        Annovar ì£¼ì„ íŒŒì¼ ë¡œë”©\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        annovar_file: str\n",
    "            Annovar ì£¼ì„ íŒŒì¼ ê²½ë¡œ (.txt, .csv)\n",
    "        label_column: str\n",
    "            Pathogenic/Benign ë¼ë²¨ ì»¬ëŸ¼ëª…\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        df: pd.DataFrame\n",
    "        \"\"\"\n",
    "        # íŒŒì¼ ë¡œë”©\n",
    "        if annovar_file.endswith('.csv'):\n",
    "            df = pd.read_csv(annovar_file)\n",
    "        else:\n",
    "            df = pd.read_csv(annovar_file, sep='\\t')\n",
    "        \n",
    "        print(f\"Loaded {len(df)} variants from {annovar_file}\")\n",
    "        return df\n",
    "    \n",
    "    def preprocess_features(self, df):\n",
    "        \"\"\"\n",
    "        19ê°œ íŠ¹ì„± ì „ì²˜ë¦¬\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "            Annovar ë°ì´í„°í”„ë ˆì„\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X: np.ndarray (n_variants, 19)\n",
    "        feature_names: list\n",
    "        \"\"\"\n",
    "        feature_columns = [\n",
    "            'SIFT4G_score', 'LRT_score', 'MutationTaster_score',\n",
    "            'MutationAssessor_score', 'FATHMM_score', 'PROVEAN_score',\n",
    "            'CADD_phred', 'integrated_fitCons_score', 'LINSIGHT',\n",
    "            'GERP++_RS', 'phyloP100way_vertebrate', 'phyloP470way_mammalian',\n",
    "            'phastCons100way_vertebrate', 'phastCons470way_mammalian',\n",
    "            'SiPhy_29way_logOdds', 'bStatistic',\n",
    "            'gnomad41_genome', 'gnomad41_exome', 'AF_patient'\n",
    "        ]\n",
    "        \n",
    "        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "        available_features = [col for col in feature_columns if col in df.columns]\n",
    "        print(f\"Using {len(available_features)} features: {available_features}\")\n",
    "        \n",
    "        X = df[available_features].copy()\n",
    "        \n",
    "        # Missing value ì²˜ë¦¬\n",
    "        # 1. '.' ë¥¼ NaNìœ¼ë¡œ ë³€í™˜\n",
    "        X = X.replace('.', np.nan)\n",
    "        \n",
    "        # 2. ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        \n",
    "        # 3. NaN ì²˜ë¦¬ ì „ëµ\n",
    "        # - ë¹ˆë„ ë°ì´í„° (gnomAD, AF): 0ìœ¼ë¡œ ì±„ì›€ (ê´€ì°° ì•ˆ ë¨ = í¬ê·€)\n",
    "        freq_cols = ['gnomad41_genome', 'gnomad41_exome', 'AF']\n",
    "        for col in freq_cols:\n",
    "            if col in X.columns:\n",
    "                X[col].fillna(0, inplace=True)\n",
    "        \n",
    "        # - ìŠ¤ì½”ì–´ ë°ì´í„°: medianìœ¼ë¡œ ì±„ì›€\n",
    "        score_cols = [col for col in X.columns if col not in freq_cols]\n",
    "        for col in score_cols:\n",
    "            if col in X.columns:\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "        \n",
    "        # 4. ì—¬ì „íˆ NaNì´ ë‚¨ì•„ìˆìœ¼ë©´ 0ìœ¼ë¡œ\n",
    "        X.fillna(0, inplace=True)\n",
    "        \n",
    "        print(f\"Feature shape: {X.shape}\")\n",
    "        print(f\"Missing values: {X.isna().sum().sum()}\")\n",
    "        \n",
    "        return X.values, available_features\n",
    "    \n",
    "    def extract_labels(self, df, label_column='ClinVar_SIG', include_vus=False):\n",
    "        \"\"\"\n",
    "        ë³‘ì›ì„± ë¼ë²¨ ì¶”ì¶œ (í•™ìŠµìš©)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "        label_column: str\n",
    "            ë³‘ì›ì„± ë¼ë²¨ ì»¬ëŸ¼ëª…\n",
    "        include_vus: bool\n",
    "            VUSë¥¼ í•™ìŠµ ë°ì´í„°ì— í¬í•¨í• ì§€ (ê¸°ë³¸ False)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_binary: np.ndarray (n_labeled_variants,)\n",
    "            1 = Pathogenic, 0 = Benign\n",
    "        labeled_idx: np.ndarray\n",
    "            ë¼ë²¨ì´ ìˆëŠ” ë³€ì´ì˜ ì¸ë±ìŠ¤ (VUS ì œì™¸)\n",
    "        \"\"\"\n",
    "        if label_column not in df.columns:\n",
    "            raise ValueError(f\"Label column '{label_column}' not found in dataframe\")\n",
    "        \n",
    "        # ë¼ë²¨ ë§¤í•‘\n",
    "        pathogenic_keywords = ['pathogenic', 'likely_pathogenic', 'Pathogenic', 'Likely_pathogenic', 'P', 'Pathogenic/Likely_pathogenic']\n",
    "        benign_keywords = ['benign', 'likely_benign', 'Benign', 'Likely_benign', 'B', 'Benign/Likely_benign']\n",
    "        vus_keywords = ['uncertain', 'VUS', 'uncertain_significance', 'Uncertain_significance', '.', 'Conflicting_classifications_of_pathogenicity']\n",
    "        \n",
    "        label_str = df[label_column].astype(str).str.lower()\n",
    "        \n",
    "        # ê° ì¹´í…Œê³ ë¦¬ êµ¬ë¶„\n",
    "        is_pathogenic = label_str.isin([k.lower() for k in pathogenic_keywords])\n",
    "        is_benign = label_str.isin([k.lower() for k in benign_keywords])\n",
    "        is_vus = label_str.isin([k.lower() for k in vus_keywords])\n",
    "        \n",
    "        # ë¼ë²¨ì´ ìˆëŠ” ë³€ì´ë§Œ (Pathogenic or Benign)\n",
    "        has_label = is_pathogenic | is_benign\n",
    "        labeled_idx = np.where(has_label)[0]\n",
    "        \n",
    "        # ì´ì§„ ë¼ë²¨ ìƒì„±\n",
    "        y_binary = np.zeros(len(df))\n",
    "        y_binary[is_pathogenic] = 1\n",
    "        y_binary[is_benign] = 0\n",
    "        \n",
    "        print(f\"\\n=== Label Statistics ===\")\n",
    "        print(f\"Pathogenic: {is_pathogenic.sum()}\")\n",
    "        print(f\"Benign: {is_benign.sum()}\")\n",
    "        print(f\"VUS (unlabeled): {is_vus.sum()}\")\n",
    "        print(f\"Unknown: {(~(is_pathogenic | is_benign | is_vus)).sum()}\")\n",
    "        print(f\"Total labeled (train): {len(labeled_idx)}\")\n",
    "        \n",
    "        if include_vus:\n",
    "            print(\"Note: Including VUS in dataset but they won't be used in training\")\n",
    "            return y_binary, None\n",
    "        else:\n",
    "            print(f\"Excluding VUS from training set\")\n",
    "            return y_binary[labeled_idx], labeled_idx\n",
    "    \n",
    "    def extract_gene_mapping(self, df, gene_column='Gene.refGene'):\n",
    "        \"\"\"\n",
    "        Variant â†’ Gene ë§¤í•‘ ì¶”ì¶œ\n",
    "        ì—¬ëŸ¬ ìœ ì „ì(A;B;C)ê°€ ìˆìœ¼ë©´ explodeí•˜ì—¬ ëª¨ë‘ ë°˜ì˜\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "        gene_column: str\n",
    "            ìœ ì „ì ì»¬ëŸ¼ëª… (Gene.refGene, Gene.ensGene ë“±)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        gene_list: list of lists\n",
    "            ê° ë³€ì´ì— í•´ë‹¹í•˜ëŠ” ìœ ì „ìëª… ë¦¬ìŠ¤íŠ¸ (ë³µìˆ˜ ê°€ëŠ¥)\n",
    "        \"\"\"\n",
    "        if gene_column not in df.columns:\n",
    "            raise ValueError(f\"Gene column '{gene_column}' not found\")\n",
    "        \n",
    "        gene_list = []\n",
    "        \n",
    "        for gene_str in df[gene_column]:\n",
    "            # AnnovarëŠ” missingì„ '.'ìœ¼ë¡œ í‘œì‹œ\n",
    "            if pd.isna(gene_str) or gene_str == '.' or gene_str == '' or str(gene_str).strip() == '.':\n",
    "                # Intergenic variant ì²˜ë¦¬\n",
    "                gene_list.append(['UNKNOWN'])\n",
    "            elif ';' in str(gene_str) or '|' in str(gene_str):\n",
    "                # ì—¬ëŸ¬ ìœ ì „ì â†’ ëª¨ë‘ ì‚¬ìš© (explode)\n",
    "                # ì„¸ë¯¸ì½œë¡ (;)ê³¼ íŒŒì´í”„(|) ëª¨ë‘ êµ¬ë¶„ìë¡œ ì²˜ë¦¬\n",
    "                gene_str_clean = str(gene_str).replace('|', ';')  # í†µì¼\n",
    "                genes = [g.strip() for g in gene_str_clean.split(';') if g.strip() and g.strip() != '.']\n",
    "                gene_list.append(genes if genes else ['UNKNOWN'])\n",
    "            else:\n",
    "                gene = str(gene_str).strip()\n",
    "                gene_list.append([gene] if gene and gene != '.' else ['UNKNOWN'])\n",
    "        \n",
    "        # ê³ ìœ  ìœ ì „ì í†µê³„\n",
    "        all_genes_flat = [g for genes in gene_list for g in genes]\n",
    "        unique_genes = set(all_genes_flat) - {'UNKNOWN'}\n",
    "        \n",
    "        print(f\"Extracted {len(unique_genes)} unique genes from {len(gene_list)} variants\")\n",
    "        print(f\"  Multi-gene variants: {sum([len(g) > 1 for g in gene_list])}\")\n",
    "        print(f\"  Unknown genes: {all_genes_flat.count('UNKNOWN')}\")\n",
    "        \n",
    "        if len(unique_genes) == 0:\n",
    "            print(\"âš  WARNING: No valid genes found! All values are '.' (missing)\")\n",
    "            print(f\"Sample values in {gene_column}: {df[gene_column].value_counts().head()}\")\n",
    "        \n",
    "        return gene_list\n",
    "    \n",
    "    def load_string_with_gene_filter(self, string_file, alias_file, my_genes, \n",
    "                                      mm_seed_genes=None, score_threshold=400, organism='9606'):\n",
    "        \"\"\"\n",
    "        ìœ ì „ì ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ PPI ë„¤íŠ¸ì›Œí¬ í•„í„°ë§ ë¡œë”© (íš¨ìœ¨ì )\n",
    "        MM seed ìœ ì „ìëŠ” PPIì— ì—†ì–´ë„ ê°•ì œë¡œ ì¶”ê°€!\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        string_file: str\n",
    "            STRING PPI íŒŒì¼ ê²½ë¡œ\n",
    "        alias_file: str\n",
    "            STRING alias íŒŒì¼ ê²½ë¡œ\n",
    "        my_genes: list\n",
    "            ë‚´ ë°ì´í„°ì— ìˆëŠ” ìœ ì „ì ë¦¬ìŠ¤íŠ¸\n",
    "        mm_seed_genes: list\n",
    "            MM í—ˆë¸Œ ìœ ì „ì ë¦¬ìŠ¤íŠ¸ (RWR seedìš©, PPIì— ê°•ì œ ì¶”ê°€)\n",
    "        score_threshold: int\n",
    "            Combined score ì„ê³„ê°’\n",
    "        organism: str\n",
    "            ìƒë¬¼ ì¢… ID (9606 = human)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        G: networkx.Graph\n",
    "            PPI ë„¤íŠ¸ì›Œí¬ (ë°ì´í„° ìœ ì „ì + MM seed)\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Loading PPI with gene filtering ===\")\n",
    "        print(f\"Target genes: {len(my_genes)}\")\n",
    "        if mm_seed_genes:\n",
    "            print(f\"MM seed genes (will be added if missing): {len(mm_seed_genes)}\")\n",
    "        \n",
    "        # ë°ì´í„° ìœ ì „ì + MM seed í†µí•©\n",
    "        all_genes_to_load = set(my_genes)\n",
    "        if mm_seed_genes:\n",
    "            all_genes_to_load.update(mm_seed_genes)\n",
    "        \n",
    "        # 1. Alias ë§¤í•‘\n",
    "        print(\"\\n[1/3] Loading alias mappings...\")\n",
    "        alias_df = pd.read_csv(\n",
    "            alias_file,\n",
    "            sep=\"\\t\", \n",
    "            header=None,\n",
    "            names=[\"STRING_id\", \"alias\", \"source\"]\n",
    "        )\n",
    "        \n",
    "        # ë‚´ ìœ ì „ì + MM ìœ ì „ì ë§¤ì¹­\n",
    "        gene_map = alias_df[alias_df[\"alias\"].isin(all_genes_to_load)].drop_duplicates(\"alias\")\n",
    "        mapped_genes = gene_map[\"alias\"].nunique()\n",
    "        print(f\"âœ“ Mapped {mapped_genes}/{len(all_genes_to_load)} genes to STRING IDs\")\n",
    "        \n",
    "        # STRING_id â†’ Gene symbol dict\n",
    "        string_to_gene = dict(zip(gene_map[\"STRING_id\"], gene_map[\"alias\"]))\n",
    "        self.protein_to_gene = string_to_gene\n",
    "        self.gene_to_protein = {v: k for k, v in string_to_gene.items()}\n",
    "        \n",
    "        # 2. PPI ë¡œë”© (í•„í„°ë§)\n",
    "        print(\"\\n[2/3] Loading PPI network...\")\n",
    "        mapped_ids = gene_map[\"STRING_id\"].tolist()\n",
    "        \n",
    "        # ì²­í¬ë¡œ ì½ìœ¼ë©´ì„œ í•„í„°ë§\n",
    "        edges = []\n",
    "        chunk_size = 100000\n",
    "        \n",
    "        with tqdm(desc=\"Filtering PPI edges\", unit=\" edges\") as pbar:\n",
    "            for chunk in pd.read_csv(string_file, sep=r'\\s+', chunksize=chunk_size, engine='python'):\n",
    "                # ë‚´ ìœ ì „ì + MM ìœ ì „ìë§Œ\n",
    "                chunk_filtered = chunk[\n",
    "                    (chunk['protein1'].isin(mapped_ids)) &\n",
    "                    (chunk['protein2'].isin(mapped_ids))\n",
    "                ]\n",
    "                \n",
    "                # Score í•„í„°ë§\n",
    "                if 'combined_score' in chunk_filtered.columns:\n",
    "                    chunk_filtered = chunk_filtered[chunk_filtered['combined_score'] >= score_threshold]\n",
    "                \n",
    "                # Edge ì¶”ì¶œ (Gene Symbolë¡œ ë³€í™˜)\n",
    "                for _, row in chunk_filtered.iterrows():\n",
    "                    p1 = string_to_gene.get(row['protein1'], row['protein1'])\n",
    "                    p2 = string_to_gene.get(row['protein2'], row['protein2'])\n",
    "                    score = float(row.get('combined_score', 1000)) / 1000.0\n",
    "                    edges.append((p1, p2, score))\n",
    "                \n",
    "                pbar.update(len(chunk))\n",
    "        \n",
    "        print(f\"âœ“ Found {len(edges)} edges\")\n",
    "        \n",
    "        # 3. NetworkX ê·¸ë˜í”„ ìƒì„±\n",
    "        print(\"\\n[3/3] Building graph...\")\n",
    "        G = nx.Graph()\n",
    "        G.add_weighted_edges_from(edges)\n",
    "        \n",
    "        # 4. MM seed ìœ ì „ì ê°•ì œ ì¶”ê°€ (PPIì— ì—†ëŠ” ê²½ìš°)\n",
    "        if mm_seed_genes:\n",
    "            added_mm = []\n",
    "            for mm_gene in mm_seed_genes:\n",
    "                if mm_gene not in G.nodes():\n",
    "                    G.add_node(mm_gene)\n",
    "                    added_mm.append(mm_gene)\n",
    "            \n",
    "            if added_mm:\n",
    "                print(f\"\\nâš  {len(added_mm)} MM genes NOT in STRING PPI:\")\n",
    "                print(f\"  {', '.join(added_mm[:10])}{'...' if len(added_mm) > 10 else ''}\")\n",
    "                print(f\"  â†’ Added as isolated nodes (will still work as RWR seeds)\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Final PPI network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def create_prediction_labels(self, df):\n",
    "        \"\"\"\n",
    "        ì˜ˆì¸¡ ìŠ¤ì½”ì–´ ê¸°ë°˜ìœ¼ë¡œ ë¼ë²¨ ìƒì„± (ClinVar ì—†ì„ ë•Œ)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df: pd.DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y_binary: np.ndarray\n",
    "        labeled_idx: np.ndarray\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Creating labels from prediction scores ===\")\n",
    "        \n",
    "        # CADD score ê¸°ë°˜ (>20 = pathogenic)\n",
    "        cadd = pd.to_numeric(df['CADD_phred'].replace('.', np.nan), errors='coerce')\n",
    "        \n",
    "        # ì—¬ëŸ¬ ì˜ˆì¸¡ ìŠ¤ì½”ì–´ ì¢…í•©\n",
    "        votes = pd.DataFrame({\n",
    "            'CADD': cadd > 20,\n",
    "            'SIFT': df['SIFT4G_pred'] == 'D',\n",
    "            'MutTaster': df['MutationTaster_pred'] == 'D'\n",
    "        })\n",
    "        \n",
    "        # 2ê°œ ì´ìƒ pathogenic ì˜ˆì¸¡ â†’ 1\n",
    "        vote_count = votes.sum(axis=1)\n",
    "        y_binary = (vote_count >= 2).astype(int)\n",
    "        \n",
    "        # ìµœì†Œ 1ê°œ ìŠ¤ì½”ì–´ë¼ë„ ìˆëŠ” ê²ƒë§Œ\n",
    "        has_score = ~cadd.isna()\n",
    "        labeled_idx = np.where(has_score)[0]\n",
    "        \n",
    "        print(f\"Labeled by prediction: {len(labeled_idx)}\")\n",
    "        print(f\"  Pathogenic (>=2 votes): {(y_binary[labeled_idx] == 1).sum()}\")\n",
    "        print(f\"  Benign (<2 votes): {(y_binary[labeled_idx] == 0).sum()}\")\n",
    "        \n",
    "        return y_binary.values[labeled_idx], labeled_idx\n",
    "    \n",
    "    def load_string_ppi(self, string_file, alias_file=None, score_threshold=400, organism='9606'):\n",
    "        \"\"\"\n",
    "        STRING PPI ë„¤íŠ¸ì›Œí¬ ë¡œë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        string_file: str\n",
    "            STRING íŒŒì¼ ê²½ë¡œ (protein.links.v12.0.txt í˜•ì‹)\n",
    "        score_threshold: int\n",
    "            Combined score ì„ê³„ê°’ (0-1000, ê¸°ë³¸ 400 = medium confidence)\n",
    "        organism: str\n",
    "            ìƒë¬¼ ì¢… ID (9606 = human)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        G: networkx.Graph\n",
    "            PPI ë„¤íŠ¸ì›Œí¬\n",
    "        \"\"\"\n",
    "        print(f\"Loading STRING PPI network from {string_file}...\")\n",
    "        \n",
    "        # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "        chunk_size = 100000\n",
    "        edges = []\n",
    "        \n",
    "        try:\n",
    "            # ì²« ì¤„ë¡œ ì»¬ëŸ¼ëª… í™•ì¸\n",
    "            first_line = pd.read_csv(string_file, sep=' ', nrows=1)\n",
    "            print(f\"Detected columns: {list(first_line.columns)}\")\n",
    "            \n",
    "            # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸°\n",
    "            for chunk in pd.read_csv(string_file, sep=' ', chunksize=chunk_size, \n",
    "                                     dtype={'protein1': str, 'protein2': str, 'combined_score': int}):\n",
    "                \n",
    "                # Score í•„í„°ë§\n",
    "                if 'combined_score' in chunk.columns:\n",
    "                    chunk = chunk[chunk['combined_score'] >= score_threshold]\n",
    "                \n",
    "                # ìœ ì „ì ì‹¬ë³¼ ì¶”ì¶œ (9606.ENSP00000XXX â†’ gene symbol ë˜ëŠ” protein ID)\n",
    "                protein1_col = chunk.columns[0]\n",
    "                protein2_col = chunk.columns[1]\n",
    "                \n",
    "                for _, row in chunk.iterrows():\n",
    "                    # organism prefix ì œê±°\n",
    "                    p1 = str(row[protein1_col]).replace(f'{organism}.', '')\n",
    "                    p2 = str(row[protein2_col]).replace(f'{organism}.', '')\n",
    "                    \n",
    "                    score = row.get('combined_score', 1000) if 'combined_score' in chunk.columns else 1000\n",
    "                    edges.append((p1, p2, score / 1000.0))\n",
    "                \n",
    "                print(f\"  Processed {len(edges)} edges...\", end='\\r')\n",
    "            \n",
    "            print(f\"\\nTotal edges after filtering: {len(edges)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {e}\")\n",
    "            print(\"Attempting alternative reading method...\")\n",
    "            \n",
    "            # ëŒ€ì•ˆ: ì§ì ‘ íŒŒì¼ ì½ê¸°\n",
    "            edges = []\n",
    "            with open(string_file, 'r') as f:\n",
    "                header = f.readline()  # í—¤ë” ê±´ë„ˆë›°ê¸°\n",
    "                \n",
    "                # íŒŒì¼ í¬ê¸°ë¡œ ì§„í–‰ë¥  ì¶”ì •\n",
    "                import os\n",
    "                file_size = os.path.getsize(string_file)\n",
    "                \n",
    "                with tqdm(total=file_size, desc=\"Reading PPI file\", unit=\"B\", unit_scale=True) as pbar:\n",
    "                    for i, line in enumerate(f):\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 3:\n",
    "                            try:\n",
    "                                score = float(parts[2])  # ëª…ì‹œì ìœ¼ë¡œ float ë³€í™˜\n",
    "                                if score >= score_threshold:\n",
    "                                    p1 = parts[0].replace(f'{organism}.', '')\n",
    "                                    p2 = parts[1].replace(f'{organism}.', '')\n",
    "                                    edges.append((p1, p2, score / 1000.0))\n",
    "                            except (ValueError, IndexError):\n",
    "                                continue\n",
    "                        \n",
    "                        if i % 10000 == 0:\n",
    "                            pbar.update(f.tell() - pbar.n)\n",
    "            \n",
    "            print(f\"\\nTotal edges: {len(edges)}\")\n",
    "        \n",
    "        # NetworkX ê·¸ë˜í”„ ìƒì„±\n",
    "        G = nx.Graph()\n",
    "        G.add_weighted_edges_from(edges)\n",
    "        \n",
    "        print(f\"Created PPI network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def create_gene_to_node_mapping(self, gene_list, ppi_graph):\n",
    "        \"\"\"\n",
    "        ìœ ì „ìëª… â†’ ë…¸ë“œ ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±\n",
    "        gene_listê°€ list of listsì¼ ê²½ìš° ì²« ë²ˆì§¸ ìœ ì „ì ì‚¬ìš©\n",
    "        (ì—¬ëŸ¬ ìœ ì „ìê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ë¥¼ ëŒ€í‘œë¡œ)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        gene_list: list of lists or list\n",
    "            ë³€ì´ë³„ ìœ ì „ìëª… ë¦¬ìŠ¤íŠ¸\n",
    "        ppi_graph: networkx.Graph\n",
    "            PPI ë„¤íŠ¸ì›Œí¬\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        node_indices: np.ndarray\n",
    "            ê° ë³€ì´ì— ëŒ€ì‘í•˜ëŠ” ë…¸ë“œ ì¸ë±ìŠ¤\n",
    "        \"\"\"\n",
    "        # PPI ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  ìœ ì „ì\n",
    "        all_genes = list(ppi_graph.nodes())\n",
    "        \n",
    "        # ìœ ì „ìëª… â†’ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "        self.gene_to_idx = {gene: idx for idx, gene in enumerate(all_genes)}\n",
    "        self.idx_to_gene = {idx: gene for gene, idx in self.gene_to_idx.items()}\n",
    "        \n",
    "        # UNKNOWN ë…¸ë“œ ì¶”ê°€ (PPIì— ì—†ëŠ” ìœ ì „ììš©)\n",
    "        unknown_idx = len(all_genes)\n",
    "        self.gene_to_idx['UNKNOWN'] = unknown_idx\n",
    "        self.idx_to_gene[unknown_idx] = 'UNKNOWN'\n",
    "        \n",
    "        # ê° ë³€ì´ì˜ ë…¸ë“œ ì¸ë±ìŠ¤\n",
    "        node_indices = []\n",
    "        for genes in gene_list:\n",
    "            # list of listsì¸ ê²½ìš° ì²« ë²ˆì§¸ ìœ ì „ì ì‚¬ìš©\n",
    "            if isinstance(genes, list):\n",
    "                gene = genes[0]\n",
    "            else:\n",
    "                gene = genes\n",
    "            \n",
    "            if gene in self.gene_to_idx:\n",
    "                node_indices.append(self.gene_to_idx[gene])\n",
    "            else:\n",
    "                node_indices.append(unknown_idx)\n",
    "        \n",
    "        print(f\"Mapped {len(node_indices)} variants to {len(set(node_indices))} unique nodes\")\n",
    "        \n",
    "        return np.array(node_indices)\n",
    "    \n",
    "    \n",
    "    def compute_rwr_from_seeds(self, ppi_graph, seed_genes, restart_prob=0.15, max_iter=100,\n",
    "                              remove_super_hubs=True, super_hub_genes=None):\n",
    "        \"\"\"\n",
    "        MM í—ˆë¸Œ ìœ ì „ìë¡œë¶€í„° Random Walk with Restart (RWR) ê³„ì‚°\n",
    "        ì§ì ‘ í–‰ë ¬ ì—°ì‚° êµ¬í˜„ìœ¼ë¡œ ì„¸ë°€í•œ ì œì–´ ê°€ëŠ¥\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ppi_graph: networkx.Graph\n",
    "        seed_genes: list\n",
    "            MM ê´€ë ¨ í—ˆë¸Œ ìœ ì „ì ë¦¬ìŠ¤íŠ¸\n",
    "        restart_prob: float\n",
    "            ì¬ì‹œì‘ í™•ë¥  (ê¸°ë³¸ 0.15)\n",
    "        max_iter: int\n",
    "            ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜\n",
    "        remove_super_hubs: bool\n",
    "            ìŠˆí¼ í—ˆë¸Œ(UBC, Histone ë“±) ì œê±° ì—¬ë¶€ (ê¸°ë³¸ True)\n",
    "        super_hub_genes: list\n",
    "            ì œê±°í•  ìŠˆí¼ í—ˆë¸Œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        rwr_scores: dict\n",
    "            {gene: rwr_score}\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Computing RWR from {len(seed_genes)} seed genes ===\")\n",
    "        \n",
    "        # ìŠˆí¼ í—ˆë¸Œ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’)\n",
    "        if super_hub_genes is None:\n",
    "            super_hub_genes = [\n",
    "                # UBC (Ubiquitin) ê³„ì—´\n",
    "                'UBC', 'UBB', 'UBA52', 'RPS27A',\n",
    "                # Histone ê³„ì—´\n",
    "                'HIST1H1A', 'HIST1H1B', 'HIST1H1C', 'HIST1H1D', 'HIST1H1E',\n",
    "                'HIST1H2AA', 'HIST1H2AB', 'HIST1H2AC', 'HIST1H2AD',\n",
    "                'HIST1H3A', 'HIST1H3B', 'HIST1H3C', 'HIST1H4A',\n",
    "                'HIST2H2AA', 'HIST2H2AB', 'HIST2H3A',\n",
    "            ]\n",
    "        \n",
    "        # ë…¸ë“œ ë¦¬ìŠ¤íŠ¸\n",
    "        all_nodes = list(ppi_graph.nodes())\n",
    "        n_nodes = len(all_nodes)\n",
    "        node_to_idx = {node: i for i, node in enumerate(all_nodes)}\n",
    "        \n",
    "        # Seed ìœ ì „ì í™•ì¸\n",
    "        valid_seeds = [g for g in seed_genes if g in node_to_idx]\n",
    "        missing_seeds = [g for g in seed_genes if g not in node_to_idx]\n",
    "        \n",
    "        print(f\"Valid seed genes in PPI: {len(valid_seeds)}/{len(seed_genes)}\")\n",
    "        \n",
    "        if missing_seeds:\n",
    "            print(f\"âš  WARNING: {len(missing_seeds)} seeds NOT in PPI graph:\")\n",
    "            print(f\"  {', '.join(missing_seeds)}\")\n",
    "        \n",
    "        if len(valid_seeds) == 0:\n",
    "            print(\"âŒ ERROR: No seed genes found in PPI network!\")\n",
    "            return {node: 0.0 for node in all_nodes}\n",
    "        \n",
    "        # ì¸ì ‘ í–‰ë ¬ (ê°€ì¤‘ì¹˜ í¬í•¨)\n",
    "        adj_matrix = nx.to_numpy_array(ppi_graph, nodelist=all_nodes, weight='weight')\n",
    "        \n",
    "        # ì „ì´ í–‰ë ¬ (ì—´ ì •ê·œí™”)\n",
    "        col_sum = adj_matrix.sum(axis=0)\n",
    "        col_sum[col_sum == 0] = 1  # Isolated node ì²˜ë¦¬\n",
    "        P = adj_matrix / col_sum\n",
    "        \n",
    "        # ì´ˆê¸° ë²¡í„° (seed genesì— ê· ë“± ë¶„ë°°)\n",
    "        r0 = np.zeros(n_nodes)\n",
    "        for seed in valid_seeds:\n",
    "            r0[node_to_idx[seed]] = 1.0 / len(valid_seeds)\n",
    "        \n",
    "        # RWR ë°˜ë³µ\n",
    "        r = r0.copy()\n",
    "        for iteration in range(max_iter):\n",
    "            r_new = (1 - restart_prob) * P.dot(r) + restart_prob * r0\n",
    "            \n",
    "            # ìˆ˜ë ´ ì²´í¬\n",
    "            if np.allclose(r, r_new, atol=1e-6):\n",
    "                print(f\"âœ“ RWR converged at iteration {iteration+1}\")\n",
    "                break\n",
    "            r = r_new\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜\n",
    "        rwr_scores = {all_nodes[i]: r[i] for i in range(n_nodes)}\n",
    "        \n",
    "        print(f\"RWR scores range: [{min(rwr_scores.values()):.6f}, {max(rwr_scores.values()):.6f}]\")\n",
    "        \n",
    "        # ìŠˆí¼ í—ˆë¸Œ ì œê±° (ì˜µì…˜)\n",
    "        if remove_super_hubs:\n",
    "            removed_count = 0\n",
    "            removed_genes = []\n",
    "            \n",
    "            for gene in rwr_scores.keys():\n",
    "                if gene in super_hub_genes:\n",
    "                    if rwr_scores[gene] > 0:\n",
    "                        removed_genes.append((gene, rwr_scores[gene]))\n",
    "                        rwr_scores[gene] = 0.0\n",
    "                        removed_count += 1\n",
    "                elif (gene.startswith('HIST') or \n",
    "                      gene.startswith('RPL') or \n",
    "                      gene.startswith('RPS')):\n",
    "                    if rwr_scores[gene] > 0:\n",
    "                        removed_genes.append((gene, rwr_scores[gene]))\n",
    "                        rwr_scores[gene] = 0.0\n",
    "                        removed_count += 1\n",
    "            \n",
    "            if removed_count > 0:\n",
    "                print(f\"âœ“ Removed {removed_count} super-hub genes from RWR scores\")\n",
    "                \n",
    "                if removed_genes:\n",
    "                    removed_genes.sort(key=lambda x: x[1], reverse=True)\n",
    "                    top_removed = removed_genes[:5]\n",
    "                    print(f\"  Top removed super-hubs:\")\n",
    "                    for gene, score in top_removed:\n",
    "                        print(f\"    {gene}: {score:.6f} â†’ 0.0\")\n",
    "        \n",
    "        return rwr_scores\n",
    "    \n",
    "    def compute_ppr_from_seeds(self, ppi_graph, seed_genes, alpha=0.15, max_iter=100,\n",
    "                              remove_super_hubs=True, super_hub_genes=None):\n",
    "        \"\"\"\n",
    "        MM í—ˆë¸Œ ìœ ì „ìë¡œë¶€í„° Personalized PageRank (PPR) ê³„ì‚°\n",
    "        \n",
    "        PPRì€ RWRê³¼ ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œ ë™ì¼í•˜ì§€ë§Œ, NetworkXì˜ pagerankë¥¼ ì‚¬ìš©í•˜ì—¬\n",
    "        ë” íš¨ìœ¨ì ì´ê³  ì•ˆì •ì ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ppi_graph: networkx.Graph\n",
    "        seed_genes: list\n",
    "            MM ê´€ë ¨ í—ˆë¸Œ ìœ ì „ì ë¦¬ìŠ¤íŠ¸\n",
    "        alpha: float\n",
    "            í…”ë ˆí¬íŠ¸ í™•ë¥  (personalizationìœ¼ë¡œ ëŒì•„ê°ˆ í™•ë¥ , ê¸°ë³¸ 0.15)\n",
    "            RWRì˜ restart_probì™€ ë™ì¼\n",
    "        max_iter: int\n",
    "            ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜\n",
    "        remove_super_hubs: bool\n",
    "            ìŠˆí¼ í—ˆë¸Œ(UBC, Histone ë“±) ì œê±° ì—¬ë¶€ (ê¸°ë³¸ True)\n",
    "        super_hub_genes: list\n",
    "            ì œê±°í•  ìŠˆí¼ í—ˆë¸Œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ppr_scores: dict\n",
    "            {gene: ppr_score}\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Computing PPR from {len(seed_genes)} seed genes ===\")\n",
    "        \n",
    "        # ìŠˆí¼ í—ˆë¸Œ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’)\n",
    "        if super_hub_genes is None:\n",
    "            super_hub_genes = [\n",
    "                # UBC (Ubiquitin) ê³„ì—´\n",
    "                'UBC', 'UBB', 'UBA52', 'RPS27A',\n",
    "                # Histone ê³„ì—´\n",
    "                'HIST1H1A', 'HIST1H1B', 'HIST1H1C', 'HIST1H1D', 'HIST1H1E',\n",
    "                'HIST1H2AA', 'HIST1H2AB', 'HIST1H2AC', 'HIST1H2AD',\n",
    "                'HIST1H3A', 'HIST1H3B', 'HIST1H3C', 'HIST1H4A',\n",
    "                'HIST2H2AA', 'HIST2H2AB', 'HIST2H3A',\n",
    "                # Ribosomal proteins (ì„ íƒì )\n",
    "                # 'RPL', 'RPS'ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë“¤ì€ ë‚˜ì¤‘ì— prefixë¡œ í•„í„°ë§\n",
    "            ]\n",
    "        \n",
    "        # ë…¸ë“œ ë¦¬ìŠ¤íŠ¸\n",
    "        all_nodes = list(ppi_graph.nodes())\n",
    "        \n",
    "        # Seed ìœ ì „ì í™•ì¸\n",
    "        valid_seeds = [g for g in seed_genes if g in ppi_graph.nodes()]\n",
    "        missing_seeds = [g for g in seed_genes if g not in ppi_graph.nodes()]\n",
    "        \n",
    "        print(f\"Valid seed genes in PPI: {len(valid_seeds)}/{len(seed_genes)}\")\n",
    "        \n",
    "        if missing_seeds:\n",
    "            print(f\"âš  WARNING: {len(missing_seeds)} seeds NOT in PPI graph:\")\n",
    "            print(f\"  {', '.join(missing_seeds)}\")\n",
    "        \n",
    "        if len(valid_seeds) == 0:\n",
    "            print(\"âŒ ERROR: No seed genes found in PPI network!\")\n",
    "            print(\"   Cannot compute PPR. Returning zero scores.\")\n",
    "            return {node: 0.0 for node in all_nodes}\n",
    "        \n",
    "        # Personalization vector ìƒì„± (seedì—ë§Œ ê· ë“± ë¶„ë°°)\n",
    "        personalization = {node: 0.0 for node in all_nodes}\n",
    "        seed_weight = 1.0 / len(valid_seeds)\n",
    "        for seed in valid_seeds:\n",
    "            personalization[seed] = seed_weight\n",
    "        \n",
    "        # NetworkX PageRank with Personalization = PPR\n",
    "        try:\n",
    "            ppr_scores = nx.pagerank(\n",
    "                ppi_graph,\n",
    "                alpha=alpha,  # teleportation probability (= RWR restart_prob)\n",
    "                personalization=personalization,\n",
    "                max_iter=max_iter,\n",
    "                tol=1e-6,\n",
    "                weight='weight'  # ê°€ì¤‘ì¹˜ ì—£ì§€ ì‚¬ìš©\n",
    "            )\n",
    "            print(f\"âœ“ PPR converged successfully\")\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            print(\"âš  Warning: PPR did not converge within max_iter, using partial results\")\n",
    "            ppr_scores = nx.pagerank(\n",
    "                ppi_graph, \n",
    "                alpha=alpha, \n",
    "                personalization=personalization,\n",
    "                max_iter=max_iter * 2,  # ë” ë§ì€ ë°˜ë³µ ì‹œë„\n",
    "                weight='weight'\n",
    "            )\n",
    "        \n",
    "        print(f\"PPR scores range: [{min(ppr_scores.values()):.6f}, {max(ppr_scores.values()):.6f}]\")\n",
    "        \n",
    "        # ğŸ”¥ ìŠˆí¼ í—ˆë¸Œ ì œê±° (ì§ˆë³‘ íŠ¹ì´ì„± ì—†ëŠ” ì¼ë°˜ì  í—ˆë¸Œ)\n",
    "        if remove_super_hubs:\n",
    "            removed_count = 0\n",
    "            removed_genes = []\n",
    "            \n",
    "            for gene in ppr_scores.keys():\n",
    "                # ì§ì ‘ ë§¤ì¹­\n",
    "                if gene in super_hub_genes:\n",
    "                    if ppr_scores[gene] > 0:\n",
    "                        removed_genes.append((gene, ppr_scores[gene]))\n",
    "                        ppr_scores[gene] = 0.0\n",
    "                        removed_count += 1\n",
    "                # Prefix ë§¤ì¹­ (Histone, Ribosomal)\n",
    "                elif (gene.startswith('HIST') or \n",
    "                      gene.startswith('RPL') or \n",
    "                      gene.startswith('RPS')):\n",
    "                    if ppr_scores[gene] > 0:\n",
    "                        removed_genes.append((gene, ppr_scores[gene]))\n",
    "                        ppr_scores[gene] = 0.0\n",
    "                        removed_count += 1\n",
    "            \n",
    "            if removed_count > 0:\n",
    "                print(f\"âœ“ Removed {removed_count} super-hub genes from PPR scores\")\n",
    "                print(f\"  (UBC, Histone, Ribosomal proteins - general cellular functions)\")\n",
    "                \n",
    "                # ì œê±°ëœ ê²ƒ ì¤‘ ë†’ì€ ì ìˆ˜ì˜€ë˜ ê²ƒ ì¶œë ¥\n",
    "                if removed_genes:\n",
    "                    removed_genes.sort(key=lambda x: x[1], reverse=True)\n",
    "                    top_removed = removed_genes[:5]\n",
    "                    print(f\"  Top removed super-hubs:\")\n",
    "                    for gene, score in top_removed:\n",
    "                        print(f\"    {gene}: {score:.6f} â†’ 0.0\")\n",
    "        \n",
    "        return ppr_scores\n",
    "    \n",
    "    def create_node_features_with_rwr_and_ppr(self, ppi_graph, rwr_scores, ppr_scores, feature_type='centrality+rwr+ppr'):\n",
    "        \"\"\"\n",
    "        ë…¸ë“œ íŠ¹ì„± ìƒì„± (RWR + PPR ìŠ¤ì½”ì–´ ëª¨ë‘ í¬í•¨)\n",
    "        \n",
    "        RWRê³¼ PPRì„ ë‘˜ ë‹¤ ì‚¬ìš©í•˜ëŠ” ì´ìœ :\n",
    "        - RWR: ì§ì ‘ êµ¬í˜„ìœ¼ë¡œ ì„¸ë°€í•œ ì œì–´\n",
    "        - PPR: NetworkX ìµœì í™”ë¡œ ì•ˆì •ì„±\n",
    "        - ë‘ ì ìˆ˜ê°€ ì¼ì¹˜í•˜ë©´ ì‹ ë¢°ë„ â†‘, ë‹¤ë¥´ë©´ ì¶”ê°€ ì •ë³´ ì œê³µ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ppi_graph: networkx.Graph\n",
    "        rwr_scores: dict\n",
    "            {gene: rwr_score}\n",
    "        ppr_scores: dict\n",
    "            {gene: ppr_score}\n",
    "        feature_type: str\n",
    "            'centrality+rwr+ppr': ì¤‘ì‹¬ì„± + RWR + PPR ìŠ¤ì½”ì–´\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        node_features: np.ndarray (n_nodes+1, 8)\n",
    "            [degree_cent, betweenness_cent, closeness_cent, eigenvector_cent, \n",
    "             clustering, degree, RWR_score, PPR_score]\n",
    "        \"\"\"\n",
    "        n_nodes = ppi_graph.number_of_nodes()\n",
    "        \n",
    "        print(\"\\n=== Computing node features (with RWR + PPR) ===\")\n",
    "        \n",
    "        # ê·¸ë˜í”„ ì¤‘ì‹¬ì„± ê³„ì‚°\n",
    "        print(\"Computing centrality metrics...\")\n",
    "        degree_cent = nx.degree_centrality(ppi_graph)\n",
    "        betweenness_cent = nx.betweenness_centrality(ppi_graph, k=min(100, n_nodes))\n",
    "        closeness_cent = nx.closeness_centrality(ppi_graph)\n",
    "        \n",
    "        try:\n",
    "            eigenvector_cent = nx.eigenvector_centrality(ppi_graph, max_iter=1000)\n",
    "        except:\n",
    "            print(\"Warning: Eigenvector centrality failed, using degree centrality instead\")\n",
    "            eigenvector_cent = degree_cent\n",
    "        \n",
    "        clustering = nx.clustering(ppi_graph)\n",
    "        \n",
    "        # íŠ¹ì„± ë²¡í„° êµ¬ì„± (RWR + PPR ë‘˜ ë‹¤!)\n",
    "        features = []\n",
    "        for node in ppi_graph.nodes():\n",
    "            features.append([\n",
    "                degree_cent[node],\n",
    "                betweenness_cent[node],\n",
    "                closeness_cent[node],\n",
    "                eigenvector_cent[node],\n",
    "                clustering[node],\n",
    "                ppi_graph.degree(node),  # raw degree\n",
    "                rwr_scores.get(node, 0.0),  # RWR score (MM ê´€ë ¨ì„± - ì§ì ‘ êµ¬í˜„)\n",
    "                ppr_scores.get(node, 0.0)   # PPR score (MM ê´€ë ¨ì„± - NetworkX êµ¬í˜„)\n",
    "            ])\n",
    "        \n",
    "        node_features = np.array(features)\n",
    "        \n",
    "        # UNKNOWN ë…¸ë“œ íŠ¹ì„± (ì¤‘ë¦½ê°’ = 0ìœ¼ë¡œ ì„¤ì •)\n",
    "        unknown_features = np.zeros((1, node_features.shape[1]))\n",
    "        unknown_features[0, -2] = 0.0  # RWR score = 0\n",
    "        unknown_features[0, -1] = 0.0  # PPR score = 0\n",
    "        node_features = np.vstack([node_features, unknown_features])\n",
    "        \n",
    "        print(f\"Node features shape: {node_features.shape}\")\n",
    "        print(f\"Features (8D): degree_cent, betweenness_cent, closeness_cent, eigenvector_cent, clustering, degree, RWR_score, PPR_score\")\n",
    "        print(f\"âœ“ Using both RWR and PPR for robustness (complementary signals)\")\n",
    "        print(f\"âœ“ UNKNOWN node features set to neutral (0.0) to avoid bias\")\n",
    "        \n",
    "        return node_features\n",
    "    \n",
    "    def create_adjacency_matrix(self, ppi_graph):\n",
    "        \"\"\"\n",
    "        ì¸ì ‘ í–‰ë ¬ ìƒì„±\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        adj_matrix: np.ndarray (n_nodes+1, n_nodes+1)\n",
    "            ë§ˆì§€ë§‰ í–‰/ì—´ì€ UNKNOWN ë…¸ë“œ (ìê¸° ìì‹ ë§Œ ì—°ê²°)\n",
    "        \"\"\"\n",
    "        n_nodes = ppi_graph.number_of_nodes()\n",
    "        \n",
    "        # NetworkXì—ì„œ ì¸ì ‘ í–‰ë ¬ ì¶”ì¶œ\n",
    "        adj_matrix = nx.to_numpy_array(ppi_graph)\n",
    "        \n",
    "        # UNKNOWN ë…¸ë“œ ì¶”ê°€ (ìê¸° ìì‹ ë§Œ ì—°ê²°)\n",
    "        unknown_row = np.zeros((1, n_nodes))\n",
    "        adj_matrix = np.vstack([adj_matrix, unknown_row])\n",
    "        \n",
    "        unknown_col = np.zeros((n_nodes + 1, 1))\n",
    "        unknown_col[-1, -1] = 1.0  # self-loop\n",
    "        adj_matrix = np.hstack([adj_matrix, unknown_col])\n",
    "        \n",
    "        print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "        \n",
    "        return adj_matrix\n",
    "\n",
    "def prepare_variant_data(annovar_file, string_file, mm_genes, \n",
    "                        label_column='CLNSIG', gene_column='Gene.refGene',\n",
    "                        alias_file=None,\n",
    "                        rwr_restart_prob=0.15, ppr_alpha=0.15,\n",
    "                        use_prediction_labels=False,\n",
    "                        exonic_only=True, remove_super_hubs=True):\n",
    "    \"\"\"\n",
    "    ì „ì²´ ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    annovar_file: str\n",
    "        Annovar ì£¼ì„ íŒŒì¼ ê²½ë¡œ\n",
    "    string_file: str\n",
    "        STRING PPI íŒŒì¼ ê²½ë¡œ\n",
    "    mm_genes: list\n",
    "        MM ê´€ë ¨ í—ˆë¸Œ ìœ ì „ì ë¦¬ìŠ¤íŠ¸ (RWR/PPR seed, PPIì— ê°•ì œ ì¶”ê°€ë¨)\n",
    "    label_column: str\n",
    "        ë³‘ì›ì„± ë¼ë²¨ ì»¬ëŸ¼ëª… (ê¸°ë³¸: 'CLNSIG')\n",
    "    gene_column: str\n",
    "        ìœ ì „ì ì»¬ëŸ¼ëª…\n",
    "    alias_file: str\n",
    "        STRING alias íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ì¶”ë¡ )\n",
    "    rwr_restart_prob: float\n",
    "        RWR ì¬ì‹œì‘ í™•ë¥  (ê¸°ë³¸ 0.15, ì§ì ‘ êµ¬í˜„)\n",
    "    ppr_alpha: float\n",
    "        PPR í…”ë ˆí¬íŠ¸ í™•ë¥  (ê¸°ë³¸ 0.15, NetworkX êµ¬í˜„)\n",
    "    remove_super_hubs: bool\n",
    "        ìŠˆí¼ í—ˆë¸Œ(UBC, Histone ë“±) ì ìˆ˜ ì œê±° ì—¬ë¶€ (ê¸°ë³¸: True)\n",
    "    use_prediction_labels: bool\n",
    "        ClinVar ì—†ì„ ë•Œ ì˜ˆì¸¡ ìŠ¤ì½”ì–´ë¡œ ë¼ë²¨ ìƒì„± ì—¬ë¶€\n",
    "    exonic_only: bool\n",
    "        Exonic variantë§Œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, y_train, X_all, y_all, node_features, adjacency_matrix,\n",
    "    node_indices_train, node_indices_all, df_all, loader\n",
    "    \"\"\"\n",
    "    \n",
    "    loader = VariantDataLoader()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"MM Pathogenic Variant Prediction - Data Preparation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Annovar ë°ì´í„° ë¡œë”©\n",
    "    df_all = loader.load_annovar_data(annovar_file, label_column)\n",
    "    \n",
    "    # 1.5. Exonic+Splicing í•„í„°ë§, Synonymous SNV ì œì™¸ (ì˜µì…˜)\n",
    "    if exonic_only and 'Func.refGene' in df_all.columns:\n",
    "        before = len(df_all)\n",
    "        \n",
    "        # Exonic OR Splicing ë³€ì´ í¬í•¨\n",
    "        functional_mask = (\n",
    "            df_all['Func.refGene'].str.contains('exonic', case=False, na=False) |\n",
    "            df_all['Func.refGene'].str.contains('splicing', case=False, na=False)\n",
    "        )\n",
    "        df_all = df_all[functional_mask].reset_index(drop=True)\n",
    "        \n",
    "        after_func = len(df_all)\n",
    "        print(f\"\\nâœ“ Filtered to exonic/splicing variants: {after_func} (from {before:,})\")\n",
    "        \n",
    "        # Synonymous SNV ì œì™¸ (ExonicFunc.refGene ì»¬ëŸ¼ ì‚¬ìš©)\n",
    "        if 'ExonicFunc.refGene' in df_all.columns:\n",
    "            before_syn = len(df_all)\n",
    "            syn_mask = df_all['ExonicFunc.refGene'].str.contains('synonymous SNV', case=False, na=False)\n",
    "            df_all = df_all[~syn_mask].reset_index(drop=True)\n",
    "            \n",
    "            print(f\"âœ“ Excluded synonymous SNV: {len(df_all)} (removed {before_syn - len(df_all)})\")\n",
    "        \n",
    "        print(f\"  Total filtering rate: {len(df_all)/before*100:.1f}%\")\n",
    "    \n",
    "    # 2. íŠ¹ì„± ì¶”ì¶œ (ì „ì²´)\n",
    "    X_all, feature_names = loader.preprocess_features(df_all)\n",
    "    \n",
    "    # 3. ë¼ë²¨ ì¶”ì¶œ\n",
    "    if use_prediction_labels:\n",
    "        print(\"\\nâš  Using prediction scores as labels (no ClinVar data)\")\n",
    "        # CADD score ê¸°ë°˜ ë¼ë²¨ ìƒì„±\n",
    "        y_all_temp, labeled_idx = loader.create_prediction_labels(df_all)\n",
    "    else:\n",
    "        # ClinVar ë¼ë²¨ ì‚¬ìš©\n",
    "        y_all_temp, labeled_idx = loader.extract_labels(df_all, label_column, include_vus=False)\n",
    "    \n",
    "    # y_all ìƒì„± (ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ -1 ì±„ìš°ê¸°)\n",
    "    y_all = np.full(len(df_all), -1)\n",
    "    if labeled_idx is not None and len(labeled_idx) > 0:\n",
    "        y_all[labeled_idx] = y_all_temp\n",
    "        \n",
    "        # í•™ìŠµìš© ë°ì´í„°\n",
    "        y_train = y_all_temp\n",
    "        X_train = X_all[labeled_idx]\n",
    "        df_train = df_all.iloc[labeled_idx].reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"\\nâŒ ERROR: No training data available!\")\n",
    "        y_train = np.array([])\n",
    "        X_train = np.array([]).reshape(0, X_all.shape[1])\n",
    "        df_train = df_all.iloc[:0]\n",
    "    \n",
    "    # 4. Gene ë§¤í•‘ (ì „ì²´ & í•™ìŠµìš©)\n",
    "    gene_list_all = loader.extract_gene_mapping(df_all, gene_column)\n",
    "    gene_list_train = loader.extract_gene_mapping(df_train, gene_column) if len(df_train) > 0 else []\n",
    "    \n",
    "    # ê³ ìœ  ìœ ì „ì ë¦¬ìŠ¤íŠ¸ (UNKNOWN ì œì™¸)\n",
    "    # gene_list_allì€ ì´ì œ list of lists\n",
    "    all_genes_flat = [g for genes in gene_list_all for g in genes]\n",
    "    unique_genes = set(all_genes_flat) - {'UNKNOWN'}\n",
    "    \n",
    "    print(f\"\\nUnique genes in data: {len(unique_genes)}\")\n",
    "    print(f\"MM seed genes: {len(mm_genes)}\")\n",
    "    \n",
    "    # 5. PPI ë„¤íŠ¸ì›Œí¬ ë¡œë”© (ë°ì´í„° ìœ ì „ì + MM seed ê°•ì œ ì¶”ê°€)\n",
    "    if alias_file:\n",
    "        ppi_graph = loader.load_string_with_gene_filter(\n",
    "            string_file=string_file,\n",
    "            alias_file=alias_file,\n",
    "            my_genes=list(unique_genes),\n",
    "            mm_seed_genes=mm_genes,  # ğŸ‘ˆ MM ìœ ì „ìë¥¼ PPIì— ê°•ì œ ì¶”ê°€!\n",
    "            score_threshold=400\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nâš  WARNING: No alias file provided. Using unfiltered PPI (may be slow).\")\n",
    "        ppi_graph = loader.load_string_ppi(string_file, alias_file=None, score_threshold=400)\n",
    "        # MM ìœ ì „ì ìˆ˜ë™ ì¶”ê°€\n",
    "        for mm_gene in mm_genes:\n",
    "            if mm_gene not in ppi_graph.nodes():\n",
    "                ppi_graph.add_node(mm_gene)\n",
    "        print(f\"âœ“ Added {len(mm_genes)} MM genes to PPI\")\n",
    "    \n",
    "    # 7. RWR + PPR ê³„ì‚° (MM í—ˆë¸Œ ìœ ì „ìë¡œë¶€í„°, ìŠˆí¼ í—ˆë¸Œ ì œê±°)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Computing Network Propagation Scores (RWR + PPR)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 7-1. RWR ê³„ì‚° (ì§ì ‘ êµ¬í˜„)\n",
    "    rwr_scores = loader.compute_rwr_from_seeds(\n",
    "        ppi_graph, mm_genes, \n",
    "        restart_prob=rwr_restart_prob,\n",
    "        remove_super_hubs=remove_super_hubs\n",
    "    )\n",
    "    \n",
    "    # 7-2. PPR ê³„ì‚° (NetworkX êµ¬í˜„)\n",
    "    ppr_scores = loader.compute_ppr_from_seeds(\n",
    "        ppi_graph, mm_genes, \n",
    "        alpha=ppr_alpha,\n",
    "        remove_super_hubs=remove_super_hubs\n",
    "    )\n",
    "    \n",
    "    # 7-3. Seed ìœ ì „ìì˜ RWR/PPR ì ìˆ˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (bias ë°©ì§€)\n",
    "    # ëª©ì : seed \"ì£¼ë³€\" ìœ ì „ì ë°œê²¬, seed ìì²´ëŠ” ì´ë¯¸ ì•Œë ¤ì§\n",
    "    # Circular reasoning ë°©ì§€ - ì´ë¯¸ ì•„ëŠ” ìœ ì „ìê°€ ë†’ì€ ì ìˆ˜ ë°›ëŠ” ê±´ ì˜ë¯¸ ì—†ìŒ\n",
    "    print(\"\\n[Seed Gene Exclusion from Ranking]\")\n",
    "    excluded_rwr = 0\n",
    "    excluded_ppr = 0\n",
    "    \n",
    "    for seed in mm_genes:\n",
    "        if seed in rwr_scores and rwr_scores[seed] > 0:\n",
    "            rwr_scores[seed] = 0.0\n",
    "            excluded_rwr += 1\n",
    "        if seed in ppr_scores and ppr_scores[seed] > 0:\n",
    "            ppr_scores[seed] = 0.0\n",
    "            excluded_ppr += 1\n",
    "    \n",
    "    if excluded_rwr > 0:\n",
    "        print(f\"âœ“ RWR: Set {excluded_rwr} seed genes to 0 (avoid circular reasoning)\")\n",
    "    if excluded_ppr > 0:\n",
    "        print(f\"âœ“ PPR: Set {excluded_ppr} seed genes to 0 (avoid circular reasoning)\")\n",
    "    \n",
    "    print(f\"  â†’ Seed genes excluded from ranking to prevent bias\")\n",
    "    print(f\"  â†’ Goal: Discover NOVEL genes around seeds, not re-discover seeds themselves\")\n",
    "    \n",
    "    # Top 10 genes by RWR (seed ì œì™¸ í›„)\n",
    "    print(f\"\\n[Top 10 genes by RWR score (seeds excluded)]\")\n",
    "    sorted_rwr = sorted(rwr_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for gene, score in sorted_rwr:\n",
    "        print(f\"  {gene}: {score:.6f}\")\n",
    "    \n",
    "    # Top 10 genes by PPR (seed ì œì™¸ í›„)\n",
    "    print(f\"\\n[Top 10 genes by PPR score (seeds excluded)]\")\n",
    "    sorted_ppr = sorted(ppr_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for gene, score in sorted_ppr:\n",
    "        print(f\"  {gene}: {score:.6f}\")\n",
    "    \n",
    "    # RWRê³¼ PPR ì ìˆ˜ ë¹„êµ\n",
    "    print(f\"\\n[Score Correlation Analysis]\")\n",
    "    common_genes = set(rwr_scores.keys()) & set(ppr_scores.keys())\n",
    "    if len(common_genes) > 0:\n",
    "        rwr_vals = np.array([rwr_scores[g] for g in common_genes])\n",
    "        ppr_vals = np.array([ppr_scores[g] for g in common_genes])\n",
    "        correlation = np.corrcoef(rwr_vals, ppr_vals)[0, 1]\n",
    "        print(f\"âœ“ RWR-PPR correlation: {correlation:.4f}\")\n",
    "        if correlation > 0.9:\n",
    "            print(f\"  â†’ High agreement (>0.9): Both methods identify similar candidates\")\n",
    "        elif correlation > 0.7:\n",
    "            print(f\"  â†’ Good agreement (0.7-0.9): Methods generally consistent\")\n",
    "        else:\n",
    "            print(f\"  â†’ Moderate agreement (<0.7): Methods provide complementary information\")\n",
    "    \n",
    "    # 8. ë…¸ë“œ ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "    node_indices_all = loader.create_gene_to_node_mapping(gene_list_all, ppi_graph)\n",
    "    node_indices_train = loader.create_gene_to_node_mapping(gene_list_train, ppi_graph)\n",
    "    \n",
    "    # 9. ë…¸ë“œ íŠ¹ì„± ìƒì„± (RWR + PPR ë‘˜ ë‹¤ í¬í•¨!)\n",
    "    node_features = loader.create_node_features_with_rwr_and_ppr(ppi_graph, rwr_scores, ppr_scores)\n",
    "    \n",
    "    # 10. ì¸ì ‘ í–‰ë ¬ ìƒì„±\n",
    "    adjacency_matrix = loader.create_adjacency_matrix(ppi_graph)\n",
    "    \n",
    "    # 11. Seed ìœ ì „ì ì—¬ë¶€ ì¶”ê°€ (ê²°ê³¼ ë¶„ì„ìš©)\n",
    "    # Note: Seed genesëŠ” RWR/PPR ì‹œì‘ì ì´ì ì•Œë ¤ì§„ MM ì—°ê´€ ìœ ì „ì\n",
    "    # ì´ í‘œì‹œëŠ” ê²°ê³¼ ë¶„ì„ ë‹¨ê³„ì—ì„œ seed vs novelì„ êµ¬ë¶„í•˜ê¸° ìœ„í•¨\n",
    "    genes_all = [genes[0] if isinstance(genes, list) else genes for genes in gene_list_all]\n",
    "    df_all['is_seed_gene'] = [g in mm_genes for g in genes_all]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Data preparation completed!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"[Training Set - Labeled only]\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape} (P={(y_train==1).sum()}, B={(y_train==0).sum()})\")\n",
    "    print(f\"  node_indices_train: {node_indices_train.shape}\")\n",
    "    print(f\"\\n[Full Dataset - Including VUS]\")\n",
    "    print(f\"  X_all: {X_all.shape}\")\n",
    "    print(f\"  y_all: {y_all.shape} (P={(y_all==1).sum()}, B={(y_all==0).sum()}, VUS={(y_all==-1).sum()})\")\n",
    "    print(f\"  node_indices_all: {node_indices_all.shape}\")\n",
    "    print(f\"  Seed genes (RWR/PPR starts): {df_all['is_seed_gene'].sum()} variants\")\n",
    "    print(f\"\\n[Graph Data]\")\n",
    "    print(f\"  node_features: {node_features.shape} (8D: centrality + RWR + PPR)\")\n",
    "    print(f\"  adjacency_matrix: {adjacency_matrix.shape}\")\n",
    "    print(f\"  Total nodes in PPI: {ppi_graph.number_of_nodes()}\")\n",
    "    print(f\"  MM seed genes in PPI: {sum([g in ppi_graph.nodes() for g in mm_genes])}/{len(mm_genes)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return X_train, y_train, X_all, y_all, node_features, adjacency_matrix, \\\n",
    "           node_indices_train, node_indices_all, df_all, ppi_graph, loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25f235-c89e-467f-9647-0330933138b9",
   "metadata": {},
   "source": [
    "# (Function) Modal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c766a21-ea4e-4976-8679-16bc6da89469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modal1_Features_Classifier:\n",
    "    \"\"\"\n",
    "    19ê°œ ì›ë³¸ í”¼ì²˜ + Classifier Probability\n",
    "    UMAPì€ ì‹œê°í™”/ë¶„ì„ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier_type='logistic', use_calibration=True, random_state=42):\n",
    "        self.classifier_type = classifier_type\n",
    "        self.use_calibration = use_calibration\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # ë¶„ë¥˜ê¸° ì„ íƒ\n",
    "        if classifier_type == 'logistic':\n",
    "            base_clf = LogisticRegression(random_state=random_state, max_iter=1000)\n",
    "        elif classifier_type == 'svm':\n",
    "            base_clf = SVC(kernel='rbf', probability=True, random_state=random_state)\n",
    "        \n",
    "        # Calibration ë˜í¼ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)\n",
    "        if use_calibration and CALIBRATION_AVAILABLE:\n",
    "            self.classifier = CalibratedClassifierCV(\n",
    "                base_clf, \n",
    "                method='sigmoid',  # ğŸ”¥ isotonic â†’ sigmoid (ë” ë¶€ë“œëŸ¬ìš´ ë³´ì •)\n",
    "                cv=3  # 3-fold CVë¡œ ë³´ì •\n",
    "            )\n",
    "            print(f\"âœ“ Using calibrated {classifier_type} classifier (sigmoid method)\")\n",
    "        else:\n",
    "            self.classifier = base_clf\n",
    "            if use_calibration and not CALIBRATION_AVAILABLE:\n",
    "                print(f\"âš  Calibration requested but not available (sklearn version issue)\")\n",
    "                print(f\"  Using uncalibrated {classifier_type} classifier\")\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # UMAPì€ ì‹œê°í™”ìš© (Fusionì—ëŠ” ì‚¬ìš© ì•ˆ í•¨!)\n",
    "        self.umap = UMAP(\n",
    "            n_components=2,\n",
    "            random_state=random_state,\n",
    "            metric='euclidean',\n",
    "            n_neighbors=30,\n",
    "            min_dist=0.0,\n",
    "            target_weight=0.5\n",
    "        )\n",
    "        self.umap_embedding = None  # ì‹œê°í™”ìš©ìœ¼ë¡œë§Œ ì €ì¥\n",
    "        \n",
    "    def fit(self, X_all, y_all, labeled_mask=None):\n",
    "        \"\"\"\n",
    "        ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_all: (n_all_samples, n_features) - VUS í¬í•¨ ì „ì²´ ì…ë ¥ í”¼ì²˜\n",
    "        y_all: (n_all_samples,) - ë¼ë²¨ (Pathogenic=1, Benign=0, VUS=-1)\n",
    "        labeled_mask: (n_all_samples,) - True/False, ë¼ë²¨ì´ ìˆëŠ” ìƒ˜í”Œë§Œ True\n",
    "        \"\"\"\n",
    "        # labeled_maskê°€ ì—†ìœ¼ë©´ y != -1ì¸ ê²ƒë“¤ë§Œ ì‚¬ìš©\n",
    "        if labeled_mask is None:\n",
    "            labeled_mask = (y_all != -1)\n",
    "        \n",
    "        labeled_count = labeled_mask.sum() if hasattr(labeled_mask, 'sum') else sum(labeled_mask)\n",
    "        \n",
    "        # ì •ê·œí™”: labeled ë°ì´í„°ë¡œë§Œ fit! (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)\n",
    "        if labeled_count > 0:\n",
    "            X_labeled = X_all[labeled_mask]\n",
    "            self.scaler.fit(X_labeled)\n",
    "            print(f\"âœ“ Scaler fitted on {labeled_count} labeled samples only (no data leakage)\")\n",
    "        else:\n",
    "            print(\"âš  Warning: No labeled data. Fitting scaler on all data.\")\n",
    "            self.scaler.fit(X_all)\n",
    "        \n",
    "        # ì „ì²´ ë°ì´í„° ë³€í™˜\n",
    "        X_scaled = self.scaler.transform(X_all)\n",
    "        \n",
    "        # 1. UMAP í•™ìŠµ (ì‹œê°í™”ìš© - P/Bë§Œìœ¼ë¡œ)\n",
    "        if labeled_count > 0:\n",
    "            X_labeled = X_scaled[labeled_mask]\n",
    "            y_labeled = y_all[labeled_mask]\n",
    "            \n",
    "            print(f\"[UMAP] Training on {labeled_count} labeled samples (for visualization)...\")\n",
    "            self.umap.fit(X_labeled, y=y_labeled)\n",
    "            \n",
    "            # ì „ì²´ ë°ì´í„° ë³€í™˜ (ì‹œê°í™”ìš©)\n",
    "            self.umap_embedding = self.umap.transform(X_scaled)\n",
    "            print(f\"âœ“ UMAP embedding created: {self.umap_embedding.shape}\")\n",
    "        else:\n",
    "            print(\"Warning: No labeled data. Using unsupervised UMAP.\")\n",
    "            self.umap_embedding = self.umap.fit_transform(X_scaled)\n",
    "        \n",
    "        # 2. Classifier í•™ìŠµ (19D + UMAP 2D = 21D íŠ¹ì„±ìœ¼ë¡œ!)\n",
    "        if labeled_count > 0:\n",
    "            X_train = X_scaled[labeled_mask]\n",
    "            y_train = y_all[labeled_mask]\n",
    "            \n",
    "            if len(np.unique(y_train)) >= 2:\n",
    "                # UMAP ì¢Œí‘œë„ í¬í•¨ (transformê³¼ ë™ì¼í•˜ê²Œ!)\n",
    "                umap_train = self.umap_embedding[labeled_mask]  # (n, 2)\n",
    "                X_train_with_umap = np.hstack([X_train, umap_train])  # (n, 21)\n",
    "                \n",
    "                print(f\"\\n[Classifier] Training on {labeled_count} labeled samples (21D features: 19D + UMAP 2D)...\")\n",
    "                self.classifier.fit(X_train_with_umap, y_train)\n",
    "                \n",
    "                # í•™ìŠµ ì •í™•ë„ í™•ì¸\n",
    "                train_acc = self.classifier.score(X_train_with_umap, y_train)\n",
    "                print(f\"âœ“ Classifier trained: P={sum(y_train==1)}, B={sum(y_train==0)}\")\n",
    "                print(f\"  Training accuracy: {train_acc:.3f}\")\n",
    "            else:\n",
    "                print(f\"âš  Warning: Only {len(np.unique(y_train))} class found. Cannot train classifier.\")\n",
    "        else:\n",
    "            print(\"âš  Warning: No labeled data for classifier training.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Fusionìš© íŠ¹ì„± ì¶”ì¶œ: ì›ë³¸ 19D + Probability\n",
    "        \n",
    "        í•µì‹¬ ë¡œì§:\n",
    "        1. ClassifierëŠ” 21D(19D + UMAP 2D)ë¡œ í•™ìŠµë¨\n",
    "        2. ë”°ë¼ì„œ predict_probaë„ 21Dë¥¼ ì…ë ¥ë°›ì•„ì•¼ í•¨\n",
    "        3. í•˜ì§€ë§Œ Fusionì—ëŠ” 20D(19D + Prob)ë§Œ ì „ë‹¬\n",
    "           â†’ UMAP ì •ë³´ëŠ” Probabilityì— ì´ë¯¸ ë°˜ì˜ë¨\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n_samples, 19) - ì›ë³¸ íŠ¹ì„±\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        summary_features: (n_samples, 20) = [19D features + P(Pathogenic)]\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # 1. UMAP ì¢Œí‘œ ê³„ì‚° (Classifier ì…ë ¥ìš©)\n",
    "        try:\n",
    "            umap_coords = self.umap.transform(X_scaled)  # (n, 2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: UMAP transform failed: {e}\")\n",
    "            print(\"Using zero coordinates\")\n",
    "            umap_coords = np.zeros((len(X), 2))\n",
    "        \n",
    "        # 2. Classifier ì…ë ¥: 19D + UMAP 2D = 21D (í•™ìŠµ ì‹œì™€ ë™ì¼!)\n",
    "        X_for_clf = np.hstack([X_scaled, umap_coords])\n",
    "        \n",
    "        # 3. Probability ê³„ì‚° (21D ì…ë ¥!)\n",
    "        try:\n",
    "            proba = self.classifier.predict_proba(X_for_clf)[:, 1]  # P(Pathogenic)\n",
    "            \n",
    "            # í™•ë¥  clipping (í¬í™” ë°©ì§€)\n",
    "            proba = np.clip(proba, 0.01, 0.99)\n",
    "            proba_values = proba.reshape(-1, 1)\n",
    "        except (AttributeError, ValueError) as e:\n",
    "            # ë¶„ë¥˜ê¸° í•™ìŠµ ì•ˆ ëœ ê²½ìš° 0.5ë¡œ\n",
    "            print(f\"Warning: Classifier prediction failed: {e}\")\n",
    "            print(\"Using 0.5 as default probability.\")\n",
    "            proba_values = np.full((len(X), 1), 0.5)\n",
    "        \n",
    "        # 4. Fusion ì…ë ¥: ì›ë³¸ 19D + Probability = 20D\n",
    "        #    (UMAPì€ Classifier ì˜ˆì¸¡ì—ë§Œ ì‚¬ìš©, Fusionì—ëŠ” ì•ˆ ë“¤ì–´ê°)\n",
    "        summary_features = np.hstack([X_scaled, proba_values])\n",
    "        # (n, 20) = [19D normalized features + P(Pathogenic)]\n",
    "        \n",
    "        return summary_features\n",
    "    \n",
    "    def fit_transform(self, X_all, y_all, labeled_mask=None):\n",
    "        \"\"\"\n",
    "        fitê³¼ transformì„ í•œ ë²ˆì—\n",
    "        \"\"\"\n",
    "        self.fit(X_all, y_all, labeled_mask)\n",
    "        return self.transform(X_all)\n",
    "\n",
    "\n",
    "# ==================== Modal 2: GCN for PPI Network ====================\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"Graph Convolutional Layer\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, X, adj):\n",
    "        \"\"\"\n",
    "        X: (n_nodes, in_features)\n",
    "        adj: (n_nodes, n_nodes) - ì •ê·œí™”ëœ ì¸ì ‘ í–‰ë ¬\n",
    "        \"\"\"\n",
    "        support = self.linear(X)\n",
    "        output = torch.matmul(adj, support)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Modal2_GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    PPI ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ GCN ëª¨ë¸\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=16, dropout=0.5):\n",
    "        super(Modal2_GCN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # GCN ë ˆì´ì–´ êµ¬ì„±\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(GCNLayer(dims[i], dims[i+1]))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X, adj):\n",
    "        \"\"\"\n",
    "        X: (n_nodes, input_dim) - ë…¸ë“œ íŠ¹ì„±\n",
    "        adj: (n_nodes, n_nodes) - ì •ê·œí™”ëœ ì¸ì ‘ í–‰ë ¬\n",
    "        \"\"\"\n",
    "        h = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = layer(h, adj)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout_layer(h)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ë ˆì´ì–´ (í™œì„±í™” ì—†ìŒ)\n",
    "        h = self.layers[-1](h, adj)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_adjacency(adj):\n",
    "        \"\"\"\n",
    "        ì¸ì ‘ í–‰ë ¬ ì •ê·œí™”: D^(-1/2) * A * D^(-1/2)\n",
    "        \"\"\"\n",
    "        adj = adj + torch.eye(adj.size(0))  # Self-loop ì¶”ê°€\n",
    "        degree = torch.sum(adj, dim=1)\n",
    "        d_inv_sqrt = torch.pow(degree, -0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "        normalized_adj = torch.matmul(torch.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "        return normalized_adj\n",
    "\n",
    "\n",
    "# ==================== Cross-Modal Attention ====================\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ë‘ ëª¨ë‹¬ ê°„ ìƒí˜¸ attention (ìƒ˜í”Œë³„ ìŠ¤ì¹¼ë¼ ê²Œì´íŠ¸ ë°©ì‹)\n",
    "    ê° ìƒ˜í”Œì´ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ ë°°ì¹˜ ê°„ ëˆ„ìˆ˜ ì—†ìŒ\n",
    "    + íˆ¬ì˜ ë ˆì´ì–´ë¡œ ìƒëŒ€ ëª¨ë‹¬ ì •ë³´ ì§ì ‘ ì£¼ì…\n",
    "    \"\"\"\n",
    "    def __init__(self, modal1_dim, modal2_dim, attention_dim=64):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        \n",
    "        # Modal 1 â†’ Modal 2 ê²Œì´íŠ¸ (ìƒ˜í”Œë³„ ìŠ¤ì¹¼ë¼)\n",
    "        self.gate_m1_to_m2 = nn.Sequential(\n",
    "            nn.Linear(modal1_dim + modal2_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(attention_dim, 1),\n",
    "            nn.Sigmoid()  # 0~1 ê²Œì´íŠ¸ ê°’\n",
    "        )\n",
    "        \n",
    "        # Modal 2 â†’ Modal 1 ê²Œì´íŠ¸ (ìƒ˜í”Œë³„ ìŠ¤ì¹¼ë¼)\n",
    "        self.gate_m2_to_m1 = nn.Sequential(\n",
    "            nn.Linear(modal1_dim + modal2_dim, attention_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(attention_dim, 1),\n",
    "            nn.Sigmoid()  # 0~1 ê²Œì´íŠ¸ ê°’\n",
    "        )\n",
    "        \n",
    "        # ì°¨ì› ë§ì¶”ê¸° ìœ„í•œ íˆ¬ì˜ ë ˆì´ì–´\n",
    "        self.proj_m2_to_m1 = nn.Linear(modal2_dim, modal1_dim)  # 16D â†’ 20D\n",
    "        self.proj_m1_to_m2 = nn.Linear(modal1_dim, modal2_dim)  # 20D â†’ 16D\n",
    "        \n",
    "    def forward(self, modal1_features, modal2_features):\n",
    "        \"\"\"\n",
    "        modal1_features: (batch_size, modal1_dim) = 20D\n",
    "        modal2_features: (batch_size, modal2_dim) = 16D\n",
    "        \n",
    "        Returns:\n",
    "            attended_m1: gate * m1 + (1-gate) * Proj(m2)\n",
    "            attended_m2: gate * m2 + (1-gate) * Proj(m1)\n",
    "        \"\"\"\n",
    "        # ë‘ ëª¨ë‹¬ ê²°í•©\n",
    "        combined = torch.cat([modal1_features, modal2_features], dim=1)\n",
    "        \n",
    "        # ìƒ˜í”Œë³„ ê²Œì´íŠ¸ ê°’ ê³„ì‚° (ë°°ì¹˜ ê°„ ë…ë¦½)\n",
    "        gate_12 = self.gate_m1_to_m2(combined)  # (B, 1) - Modal1ì´ Modal2 ìˆ˜ìš©ë„\n",
    "        gate_21 = self.gate_m2_to_m1(combined)  # (B, 1) - Modal2ê°€ Modal1 ìˆ˜ìš©ë„\n",
    "        \n",
    "        # ìƒëŒ€ ëª¨ë‹¬ ì •ë³´ íˆ¬ì˜\n",
    "        m2_projected = self.proj_m2_to_m1(modal2_features)  # (B, 20) - Modal2ë¥¼ Modal1 ê³µê°„ìœ¼ë¡œ\n",
    "        m1_projected = self.proj_m1_to_m2(modal1_features)  # (B, 16) - Modal1ì„ Modal2 ê³µê°„ìœ¼ë¡œ\n",
    "        \n",
    "        # ê²Œì´íŠ¸ë¥¼ ì´ìš©í•œ ì •ë³´ ìœµí•©\n",
    "        # gate=1: ìê¸° ìì‹ ë§Œ, gate=0: ìƒëŒ€ë°© ì •ë³´ë§Œ\n",
    "        attended_m1 = gate_21 * modal1_features + (1 - gate_21) * m2_projected\n",
    "        attended_m2 = gate_12 * modal2_features + (1 - gate_12) * m1_projected\n",
    "        \n",
    "        return attended_m1, attended_m2\n",
    "\n",
    "\n",
    "# ==================== Cross-Modal Fusion MLP ====================\n",
    "class CrossModalFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-modal attentionì„ ì ìš©í•œ í›„ ìœµí•©í•˜ì—¬ ìµœì¢… ë­í‚¹ ìŠ¤ì½”ì–´ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    def __init__(self, modal1_dim, modal2_dim, attention_dim=64, hidden_dims=[128, 64, 32], dropout=0.3):\n",
    "        super(CrossModalFusionMLP, self).__init__()\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = CrossModalAttention(modal1_dim, modal2_dim, attention_dim)\n",
    "        \n",
    "        # Fusion network\n",
    "        # ì›ë³¸ + attended íŠ¹ì„± ëª¨ë‘ ì‚¬ìš©\n",
    "        input_dim = (modal1_dim * 2) + (modal2_dim * 2)  # ì›ë³¸ + attended\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims + [1]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:  # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œì™¸\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ Sigmoid ì œê±° (BCEWithLogitsLoss ì‚¬ìš©ì„ ìœ„í•´)\n",
    "        # layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, modal1_features, modal2_features):\n",
    "        \"\"\"\n",
    "        modal1_features: (batch_size, modal1_dim) = 20D\n",
    "        modal2_features: (batch_size, modal2_dim) = 16D\n",
    "        \n",
    "        Returns: logits (not probabilities!)\n",
    "        \"\"\"\n",
    "        # Cross-modal attention ì ìš©\n",
    "        attended_m1, attended_m2 = self.cross_attention(modal1_features, modal2_features)\n",
    "        \n",
    "        # ì›ë³¸ íŠ¹ì„± + attended íŠ¹ì„±ì„ ëª¨ë‘ ê²°í•©\n",
    "        fused = torch.cat([\n",
    "            modal1_features, attended_m1,  # Modal 1: 20D + 20D\n",
    "            modal2_features, attended_m2   # Modal 2: 16D + 16D\n",
    "        ], dim=1)  # Total: 72D\n",
    "        \n",
    "        score = self.network(fused)\n",
    "        return score.squeeze(-1)\n",
    "\n",
    "\n",
    "# ==================== Late Fusion MLP (ê¸°ì¡´ ë²„ì „ ìœ ì§€) ====================\n",
    "class LateFusionMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ë‘ ëª¨ë‹¬ì˜ íŠ¹ì„±ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ë­í‚¹ ìŠ¤ì½”ì–´ ì¶œë ¥ (ë‹¨ìˆœ concatenation)\n",
    "    \"\"\"\n",
    "    def __init__(self, modal1_dim, modal2_dim, hidden_dims=[128, 64, 32], dropout=0.3):\n",
    "        super(LateFusionMLP, self).__init__()\n",
    "        \n",
    "        input_dim = modal1_dim + modal2_dim\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims + [1]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:  # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œì™¸\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ Sigmoid ì œê±° (BCEWithLogitsLoss ì‚¬ìš©)\n",
    "        # layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, modal1_features, modal2_features):\n",
    "        \"\"\"\n",
    "        modal1_features: (batch_size, modal1_dim)\n",
    "        modal2_features: (batch_size, modal2_dim)\n",
    "        \n",
    "        Returns: logits (not probabilities!)\n",
    "        \"\"\"\n",
    "        # Late fusion: ë‹¨ìˆœ concatenation\n",
    "        fused = torch.cat([modal1_features, modal2_features], dim=1)\n",
    "        score = self.network(fused)\n",
    "        return score.squeeze(-1)\n",
    "\n",
    "\n",
    "# ==================== í†µí•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ====================\n",
    "class MultimodalRankingModel:\n",
    "    \"\"\"\n",
    "    ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í†µí•©í•œ ë©€í‹°ëª¨ë‹¬ ë­í‚¹ ëª¨ë¸\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 # Modal 1 íŒŒë¼ë¯¸í„°\n",
    "                 n_input_features=19,  # ì…ë ¥ íŠ¹ì„± ê°œìˆ˜\n",
    "                 classifier_type='logistic',\n",
    "                 use_calibration=True,  # Classifier í™•ë¥  ë³´ì • ì‚¬ìš© ì—¬ë¶€\n",
    "                 # Modal 2 íŒŒë¼ë¯¸í„° (GCN)\n",
    "                 node_feature_dim=None,\n",
    "                 gcn_hidden_dims=[64, 32],\n",
    "                 gcn_output_dim=16,\n",
    "                 # Fusion íŒŒë¼ë¯¸í„°\n",
    "                 use_cross_attention=True,  # Cross-modal attention ì‚¬ìš© ì—¬ë¶€\n",
    "                 attention_dim=64,\n",
    "                 mlp_hidden_dims=[128, 64, 32],\n",
    "                 dropout=0.3,\n",
    "                 # ğŸ”¥ Score saturation ë°©ì§€\n",
    "                 temperature=1.0,  # Temperature scaling (>1ì´ë©´ í™•ë¥ ì´ ë¶€ë“œëŸ¬ì›Œì§)\n",
    "                 logits_clip=10.0,  # Logits clipping ë²”ìœ„\n",
    "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \n",
    "        self.device = device\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        self.n_input_features = n_input_features\n",
    "        self.temperature = temperature  # ğŸ”¥ Temperature scaling\n",
    "        self.logits_clip = logits_clip  # ğŸ”¥ Logits clipping\n",
    "        \n",
    "        # Modal 1: ì›ë³¸ íŠ¹ì„± + Calibrated Classifier\n",
    "        self.modal1 = Modal1_Features_Classifier(\n",
    "            classifier_type=classifier_type,\n",
    "            use_calibration=use_calibration\n",
    "        )\n",
    "        \n",
    "        # Modal 2: GCN\n",
    "        if node_feature_dim is not None:\n",
    "            self.modal2_gcn = Modal2_GCN(\n",
    "                input_dim=node_feature_dim,\n",
    "                hidden_dims=gcn_hidden_dims,\n",
    "                output_dim=gcn_output_dim,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "        else:\n",
    "            self.modal2_gcn = None\n",
    "        \n",
    "        # Fusion MLP (Cross-attention ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì„ íƒ)\n",
    "        modal1_dim = n_input_features + 1  # 19D + P(Pathogenic) = 20D\n",
    "        modal2_dim = gcn_output_dim\n",
    "        \n",
    "        if use_cross_attention:\n",
    "            self.fusion_mlp = CrossModalFusionMLP(\n",
    "                modal1_dim=modal1_dim,\n",
    "                modal2_dim=modal2_dim,\n",
    "                attention_dim=attention_dim,\n",
    "                hidden_dims=mlp_hidden_dims,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "        else:\n",
    "            self.fusion_mlp = LateFusionMLP(\n",
    "                modal1_dim=modal1_dim,\n",
    "                modal2_dim=modal2_dim,\n",
    "                hidden_dims=mlp_hidden_dims,\n",
    "                dropout=dropout\n",
    "            ).to(device)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Multimodal Ranking Model Initialized\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Modal 1: {n_input_features}D features â†’ Classifier (21D: 19D+UMAP) â†’ {modal1_dim}D\")\n",
    "        print(f\"  â†’ Classifier learns with: 19D original + 2D UMAP = 21D\")\n",
    "        print(f\"  â†’ Fusion receives: 19D original + 1D Probability = 20D\")\n",
    "        print(f\"  â†’ UMAP info is embedded in Probability!\")\n",
    "        print(f\"Modal 2: GCN ({node_feature_dim}D â†’ {gcn_output_dim}D)\")\n",
    "        print(f\"Fusion: Cross-attention={'ON' if use_cross_attention else 'OFF'} (Fixed: No batch leakage)\")\n",
    "        print(f\"Total fusion input: {modal1_dim}D + {modal2_dim}D = {modal1_dim + modal2_dim}D\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    def train_modal1(self, X_all, y_all, labeled_mask=None):\n",
    "        \"\"\"\n",
    "        Modal 1 í•™ìŠµ (VUS í¬í•¨ ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_all: (n_all, n_features) - VUS í¬í•¨ ì „ì²´ ë³€ì´ (19D)\n",
    "        y_all: (n_all,) - ë¼ë²¨ (VUSëŠ” -1)\n",
    "        labeled_mask: (n_all,) - ë¼ë²¨ ìˆëŠ” ìƒ˜í”Œë§Œ True (Noneì´ë©´ ìë™ ìƒì„±)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training Modal 1 (Features + Classifier)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        self.modal1.fit(X_all, y_all, labeled_mask)\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    def train_fusion(self, \n",
    "                     X_tabular, \n",
    "                     node_features, \n",
    "                     adjacency_matrix, \n",
    "                     node_indices,\n",
    "                     y_binary, \n",
    "                     epochs=100, \n",
    "                     lr=0.001, \n",
    "                     batch_size=32):\n",
    "        \"\"\"\n",
    "        Fusion MLP í•™ìŠµ (ì´ì§„ ë¶„ë¥˜)\n",
    "        \n",
    "        X_tabular: (n_samples, 19) - Modal 1 ì…ë ¥ (ì›ë³¸ íŠ¹ì„±)\n",
    "        node_features: (n_nodes, feature_dim) - ê·¸ë˜í”„ ë…¸ë“œ íŠ¹ì„±\n",
    "        adjacency_matrix: (n_nodes, n_nodes) - ì¸ì ‘ í–‰ë ¬\n",
    "        node_indices: (n_samples,) - ê° ìƒ˜í”Œì— í•´ë‹¹í•˜ëŠ” ë…¸ë“œ ì¸ë±ìŠ¤\n",
    "        y_binary: (n_samples,) - ì´ì§„ ë¼ë²¨ (0 or 1)\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training Fusion Model\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Modal 1 íŠ¹ì„± ì¶”ì¶œ (19D + Probability)\n",
    "        modal1_features = self.modal1.transform(X_tabular)  # (n, 20)\n",
    "        modal1_features = torch.FloatTensor(modal1_features).to(self.device)\n",
    "        \n",
    "        print(f\"Modal 1 features: {modal1_features.shape}\")\n",
    "        \n",
    "        # Modal 2 íŠ¹ì„± ì¶”ì¶œ (GCN)\n",
    "        node_features_tensor = torch.FloatTensor(node_features).to(self.device)\n",
    "        adj_tensor = torch.FloatTensor(adjacency_matrix).to(self.device)\n",
    "        adj_normalized = Modal2_GCN.normalize_adjacency(adj_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            all_node_embeddings = self.modal2_gcn(node_features_tensor, adj_normalized)\n",
    "        \n",
    "        modal2_features = all_node_embeddings[node_indices]\n",
    "        \n",
    "        print(f\"Modal 2 features: {modal2_features.shape}\")\n",
    "        \n",
    "        # íƒ€ê²Ÿ ì¤€ë¹„ (ì´ì§„ ë¼ë²¨)\n",
    "        y_tensor = torch.FloatTensor(y_binary).to(self.device)\n",
    "        \n",
    "        # í•™ìŠµ (Weight decay ì¶”ê°€ë¡œ regularization ê°•í™”)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.fusion_mlp.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=1e-4  # ğŸ”¥ L2 regularization ì¶”ê°€\n",
    "        )\n",
    "        criterion = nn.BCEWithLogitsLoss()  # Logits ì§ì ‘ ì‚¬ìš© (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )\n",
    "        \n",
    "        dataset_size = len(y_binary)\n",
    "        num_batches = (dataset_size + batch_size - 1) // batch_size  # ceil\n",
    "        \n",
    "        print(f\"\\nTraining for {epochs} epochs (batch_size={batch_size}, {num_batches} batches)...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.fusion_mlp.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ\n",
    "            indices = torch.randperm(dataset_size)\n",
    "            for i in range(0, dataset_size, batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                \n",
    "                batch_modal1 = modal1_features[batch_indices]\n",
    "                batch_modal2 = modal2_features[batch_indices]\n",
    "                batch_y = y_tensor[batch_indices]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                logits = self.fusion_mlp(batch_modal1, batch_modal2)  # logits (not proba)\n",
    "                loss = criterion(logits, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = epoch_loss / num_batches  # ë°°ì¹˜ë‹¹ í‰ê·  ì†ì‹¤\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def predict(self, X_tabular, node_features, adjacency_matrix, node_indices):\n",
    "        \"\"\"\n",
    "        ìµœì¢… ë­í‚¹ ìŠ¤ì½”ì–´ ì˜ˆì¸¡ (Raw Logits)\n",
    "        \n",
    "        ğŸ¯ í•µì‹¬ ê°œë…:\n",
    "        - Logits = ëª¨ë¸ì˜ \"ìƒ ì‹œê·¸ë„\" (raw confidence)\n",
    "        - ë†’ì„ìˆ˜ë¡ positive classì— ê°€ê¹Œì›€\n",
    "        - Rankingì—ëŠ” logitsë¥¼ ì§ì ‘ ì‚¬ìš©! (ìƒëŒ€ì  ìˆœì„œë§Œ ì¤‘ìš”)\n",
    "        - í™•ë¥ ì€ í•´ì„ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (predict_proba ë©”ì„œë“œ ì°¸ì¡°)\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Raw logits (ranking score)\n",
    "        \"\"\"\n",
    "        self.fusion_mlp.eval()\n",
    "        \n",
    "        # Modal 1 íŠ¹ì„± (19D + Probability)\n",
    "        modal1_features = self.modal1.transform(X_tabular)\n",
    "        modal1_features = torch.FloatTensor(modal1_features).to(self.device)\n",
    "        \n",
    "        # Modal 2 íŠ¹ì„±\n",
    "        node_features_tensor = torch.FloatTensor(node_features).to(self.device)\n",
    "        adj_tensor = torch.FloatTensor(adjacency_matrix).to(self.device)\n",
    "        adj_normalized = Modal2_GCN.normalize_adjacency(adj_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            all_node_embeddings = self.modal2_gcn(node_features_tensor, adj_normalized)\n",
    "            modal2_features = all_node_embeddings[node_indices]\n",
    "            \n",
    "            # ğŸ”¥ Logits ì˜ˆì¸¡ (ì´ê²Œ ranking score!)\n",
    "            logits = self.fusion_mlp(modal1_features, modal2_features)\n",
    "        \n",
    "        # Logitsë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (rankingì— ìµœì )\n",
    "        return logits.cpu().numpy().flatten()\n",
    "    \n",
    "    def predict_proba(self, X_tabular, node_features, adjacency_matrix, node_indices):\n",
    "        \"\"\"\n",
    "        í™•ë¥  ì˜ˆì¸¡ (í•´ì„ìš©)\n",
    "        \n",
    "        ì´ ë©”ì„œë“œëŠ” ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ 0~1 í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "        ì‹¤ì œ rankingì—ëŠ” predict()ì˜ logitsë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Sigmoid í™•ë¥  (0~1)\n",
    "        \"\"\"\n",
    "        self.fusion_mlp.eval()\n",
    "        \n",
    "        # Modal 1 íŠ¹ì„±\n",
    "        modal1_features = self.modal1.transform(X_tabular)\n",
    "        modal1_features = torch.FloatTensor(modal1_features).to(self.device)\n",
    "        \n",
    "        # Modal 2 íŠ¹ì„±\n",
    "        node_features_tensor = torch.FloatTensor(node_features).to(self.device)\n",
    "        adj_tensor = torch.FloatTensor(adjacency_matrix).to(self.device)\n",
    "        adj_normalized = Modal2_GCN.normalize_adjacency(adj_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            all_node_embeddings = self.modal2_gcn(node_features_tensor, adj_normalized)\n",
    "            modal2_features = all_node_embeddings[node_indices]\n",
    "            \n",
    "            # Logits ì˜ˆì¸¡\n",
    "            logits = self.fusion_mlp(modal1_features, modal2_features)\n",
    "            \n",
    "            # ğŸ”¥ Temperature scaling (í™•ë¥ ì„ ë¶€ë“œëŸ½ê²Œ)\n",
    "            scaled_logits = logits / self.temperature\n",
    "            \n",
    "            # Sigmoidë¡œ í™•ë¥  ë³€í™˜\n",
    "            probabilities = torch.sigmoid(scaled_logits)\n",
    "        \n",
    "        return probabilities.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "# ==================== Helper Functions ====================\n",
    "def filter_superhubs(df, gene_column='Gene.refGene', verbose=True):\n",
    "    \"\"\"\n",
    "    ìˆ˜í¼í—ˆë¸Œ ìœ ì „ì í•„í„°ë§\n",
    "    \n",
    "    UBC, HSP90AA1, Histone, Ribosomal ë‹¨ë°±ì§ˆ ë“± \n",
    "    ì¼ë°˜ì ì¸ ì„¸í¬ ê¸°ëŠ¥ ê´€ë ¨ ìˆ˜í¼í—ˆë¸Œ ìœ ì „ì ì œê±°\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "    gene_column: str, ìœ ì „ì ì´ë¦„ ì»¬ëŸ¼\n",
    "    verbose: bool, ì¶œë ¥ ì—¬ë¶€\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    filtered_df: ìˆ˜í¼í—ˆë¸Œê°€ ì œê±°ëœ DataFrame\n",
    "    \"\"\"\n",
    "    # ìˆ˜í¼í—ˆë¸Œ ìœ ì „ì ë¦¬ìŠ¤íŠ¸\n",
    "    SUPER_HUB_GENES = [\n",
    "        # UBC (Ubiquitin) ê³„ì—´\n",
    "        'UBC', 'UBB', 'UBA52', 'RPS27A',\n",
    "        # HSP (Heat Shock Protein) ê³„ì—´\n",
    "        'HSP90AA1', 'HSP90AB1', 'HSPA8', 'HSPA1A', 'HSPA1B',\n",
    "        'HSPA5', 'HSPA9', 'HSP90B1', 'HSPD1', 'HSPE1',\n",
    "        # Actin/Tubulin\n",
    "        'ACTB', 'ACTG1', 'TUBA1A', 'TUBA1B', 'TUBB',\n",
    "    ]\n",
    "    \n",
    "    # ëª…ì‹œì  ë¦¬ìŠ¤íŠ¸\n",
    "    mask_explicit = df[gene_column].isin(SUPER_HUB_GENES)\n",
    "    \n",
    "    # íŒ¨í„´ ë§¤ì¹­ (Histone, Ribosomal)\n",
    "    mask_hist = df[gene_column].str.startswith('HIST', na=False)\n",
    "    mask_rpl = df[gene_column].str.startswith('RPL', na=False)\n",
    "    mask_rps = df[gene_column].str.startswith('RPS', na=False)\n",
    "    \n",
    "    # ì „ì²´ ìˆ˜í¼í—ˆë¸Œ ë§ˆìŠ¤í¬\n",
    "    superhub_mask = mask_explicit | mask_hist | mask_rpl | mask_rps\n",
    "    \n",
    "    # í•„í„°ë§\n",
    "    filtered_df = df[~superhub_mask].copy()\n",
    "    removed_count = superhub_mask.sum()\n",
    "    \n",
    "    if verbose and removed_count > 0:\n",
    "        print(f\"  ğŸ”¥ Filtered out {removed_count} super-hub variants\")\n",
    "        removed_df = df[superhub_mask]\n",
    "        removed_genes = removed_df[gene_column].value_counts().head(5)\n",
    "        if len(removed_genes) > 0:\n",
    "            print(f\"     Top removed genes: {', '.join(removed_genes.index.tolist())}\")\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de97d1e-28ec-48e1-bad2-114fe10c13c4",
   "metadata": {},
   "source": [
    "#  Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ceb0cd-ed03-409c-9fcd-b50d4863a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Configuration ====================\n",
    "\n",
    "# MM-related hub gene list (seed genes for the network modality)\n",
    "mm_genes = [\n",
    "    'CRBN', 'FGFR3', 'TP53', 'KRAS', 'TNFRSF17', 'NRAS', 'LIG4', 'BRAF', 'CXCR4', \n",
    "    'XPO1', 'CD38', 'RBX1', 'CUL4A', 'PSMB5', 'DDB1', 'BCL2', 'FDPS', 'TUBB4A'\n",
    "] \n",
    "# open_target_0.6  # optional: genes from OpenTargets with score â‰¥ 0.6\n",
    "\n",
    "# File path settings\n",
    "annovar_file = r\"clnvr140401_test_code.csv\"\n",
    "string_file = r\"9606.protein.links.full.v12.0.txt\"\n",
    "alias_file = r\"9606.protein.aliases.v12.0.txt\"\n",
    "\n",
    "CALIBRATION_AVAILABLE = True \n",
    "\n",
    "# Hyperparameters\n",
    "EXONIC_ONLY = True          # use exonic variants only\n",
    "USE_CLNSIG = True           # use ClinVar labels\n",
    "RWR_RESTART_PROB = 0.15     # restart probability for RWR (Function)\n",
    "PPR_ALPHA = 0.15            # teleportation probability for PPR (NetworkX)\n",
    "STRING_THRESHOLD = 400      # minimum STRING combined score to keep an edge\n",
    "REMOVE_SUPER_HUBS = True    # remove super-hub genes (e.g., UBC, histones) to improve disease specificity\n",
    "\n",
    "# Model parameters\n",
    "CLASSIFIER_TYPE = 'logistic'  # 'logistic' or 'svm'\n",
    "USE_CROSS_ATTENTION = True    # enable cross-modal attention\n",
    "GCN_HIDDEN = [64, 32]         # GCN hidden layer sizes\n",
    "GCN_OUTPUT = 16               # GCN output dimension\n",
    "MLP_HIDDEN = [128, 64, 32]    # fusion MLP hidden layer sizes\n",
    "EPOCHS = 100                  # number of training epochs\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Score saturation control\n",
    "TEMPERATURE = 3.5             # temperature scaling (>1 makes probabilities smoother)\n",
    "LOGITS_CLIP = 10.0            # logits clipping range (-10 to +10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677ca18-eb8c-474e-9979-cc0f6d4f99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================== MM Pathogenic Variant Prediction ====================\n",
    "í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸: ë°ì´í„° ë¡œë”© â†’ ëª¨ë¸ í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ ë¶„ì„\n",
    "========================================================================== \n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     MM Pathogenic Variant Prediction - Integrated Pipeline       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ==================== Step 1: Data Preparation ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: Data Preparation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    X_train, y_train, X_all, y_all, node_feat, adj, \\\n",
    "        node_idx_train, node_idx_all, df_all, ppi_graph, loader = prepare_variant_data(\n",
    "        annovar_file=annovar_file,\n",
    "        string_file=string_file,\n",
    "        mm_genes=mm_genes,\n",
    "        alias_file=alias_file,\n",
    "        label_column='CLNSIG',\n",
    "        gene_column='Gene.refGene',\n",
    "        rwr_restart_prob=RWR_RESTART_PROB,  # RWR (ì§ì ‘ êµ¬í˜„)\n",
    "        ppr_alpha=PPR_ALPHA,                # PPR (NetworkX)\n",
    "        use_prediction_labels=not USE_CLNSIG,\n",
    "        exonic_only=EXONIC_ONLY,\n",
    "        remove_super_hubs=REMOVE_SUPER_HUBS  # ğŸ”¥ ìŠˆí¼ í—ˆë¸Œ ì œê±°\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR during data preparation: {e}\")\n",
    "    raise\n",
    "\n",
    "# ë°ì´í„° ê²€ì¦\n",
    "print(f\"\\n[Data Validation]\")\n",
    "print(f\"âœ“ Training samples: {len(y_train)}\")\n",
    "print(f\"âœ“ Total samples (including VUS): {len(y_all)}\")\n",
    "print(f\"âœ“ VUS count: {(y_all == -1).sum()}\")\n",
    "print(f\"âœ“ Input features: {X_all.shape[1]}D\")\n",
    "print(f\"âœ“ Graph nodes: {node_feat.shape[0]}\")\n",
    "print(f\"âœ“ Graph node features: {node_feat.shape[1]}D\")\n",
    "\n",
    "# ì°¨ì› ê²€ì¦\n",
    "if X_all.shape[1] < 19:\n",
    "    print(f\"\\nâš  WARNING: Expected 19 features, got {X_all.shape[1]}\")\n",
    "    print(\"Some features may be missing from your Annovar file\")\n",
    "    print(\"Model will use available features\")\n",
    "\n",
    "if len(y_train) == 0:\n",
    "    print(\"\\nâŒ ERROR: No labeled training data!\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"1. CLNSIG column has valid values\")\n",
    "    print(\"2. Or set USE_CLNSIG=False to use prediction scores\")\n",
    "    raise ValueError(\"No training data available\")\n",
    "\n",
    "if len(np.unique(y_train)) < 2:\n",
    "    print(\"\\nâŒ ERROR: Need at least 2 classes (P and B) for training!\")\n",
    "    raise ValueError(\"Insufficient class diversity\")\n",
    "\n",
    "print(\"\\nâœ“ Data preparation successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f0efc-dd1a-42af-a6ca-800e417dab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 2: Model Initialization ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: Model Initialization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = MultimodalRankingModel(\n",
    "    n_input_features=X_all.shape[1],  # 19\n",
    "    classifier_type=CLASSIFIER_TYPE,\n",
    "    node_feature_dim=node_feat.shape[1],  # 7 (centrality + RWR)\n",
    "    gcn_hidden_dims=GCN_HIDDEN,\n",
    "    gcn_output_dim=GCN_OUTPUT,\n",
    "    use_cross_attention=USE_CROSS_ATTENTION,\n",
    "    attention_dim=64,\n",
    "    mlp_hidden_dims=MLP_HIDDEN,\n",
    "    dropout=0.3,\n",
    "    temperature=TEMPERATURE,      # ğŸ”¥ Score saturation ë°©ì§€\n",
    "    logits_clip=LOGITS_CLIP       # ğŸ”¥ Logits clipping\n",
    ")\n",
    "\n",
    "# ==================== Step 3: Modal 1 Training ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: Training Modal 1 (Features + Classifier)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# VUS í¬í•¨ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ (UMAPì€ ì‹œê°í™”ìš©)\n",
    "labeled_mask = (y_all != -1)\n",
    "model.train_modal1(X_all, y_all, labeled_mask=labeled_mask)\n",
    "\n",
    "# ğŸ”¥ ë””ë²„ê¹…: Classifier ì ìˆ˜ í™•ì¸\n",
    "print(\"\\n[DEBUG] Modal 1 Scores Check:\")\n",
    "if hasattr(model.modal1, 'rescaled_scores_'):\n",
    "    scores = model.modal1.rescaled_scores_\n",
    "    print(f\"  Unique values: {len(np.unique(scores))}\")\n",
    "    print(f\"  Range: [{scores.min():.6f}, {scores.max():.6f}]\")\n",
    "    print(f\"  Sample (first 10): {scores[:10]}\")\n",
    "    df_all[\"modal1_score\"] = scores\n",
    "else:\n",
    "    print(\"  âš ï¸ rescaled_scores_ not found!\")\n",
    "\n",
    "# ğŸ”¥ ë””ë²„ê¹…: transform í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n[DEBUG] Transform Test:\")\n",
    "test_features = model.modal1.transform(X_all[:5])\n",
    "print(f\"  Output shape: {test_features.shape}\")  # (5, 20) ì´ì–´ì•¼ í•¨\n",
    "print(f\"  Sample output:\")\n",
    "print(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e89159-19ce-448d-879c-8547da16df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 4: Visualization (UMAP) ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: UMAP Visualization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # UMAP ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°\n",
    "    umap_embedding = model.modal1.umap_embedding\n",
    "    \n",
    "    if umap_embedding is not None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Pathogenic (ë¹¨ê°•)\n",
    "        pathogenic_mask = (y_all == 1)\n",
    "        ax.scatter(umap_embedding[pathogenic_mask, 0], \n",
    "                   umap_embedding[pathogenic_mask, 1],\n",
    "                   c='red', label=f'Pathogenic (n={pathogenic_mask.sum()})', \n",
    "                   alpha=0.6, s=30, edgecolors='darkred', linewidth=0.5)\n",
    "        \n",
    "        # Benign (íŒŒë‘)\n",
    "        benign_mask = (y_all == 0)\n",
    "        ax.scatter(umap_embedding[benign_mask, 0], \n",
    "                   umap_embedding[benign_mask, 1],\n",
    "                   c='blue', label=f'Benign (n={benign_mask.sum()})', \n",
    "                   alpha=0.6, s=30, edgecolors='darkblue', linewidth=0.5)\n",
    "        \n",
    "        # VUS (íšŒìƒ‰)\n",
    "        vus_mask = (y_all == -1)\n",
    "        ax.scatter(umap_embedding[vus_mask, 0], \n",
    "                   umap_embedding[vus_mask, 1],\n",
    "                   c='gray', label=f'VUS (n={vus_mask.sum()})', \n",
    "                   alpha=0.3, s=20, edgecolors='black', linewidth=0.3)\n",
    "        \n",
    "        ax.set_xlabel('UMAP Component 1', fontsize=12)\n",
    "        ax.set_ylabel('UMAP Component 2', fontsize=12)\n",
    "        ax.set_title('UMAP Embedding Space: Pathogenic vs Benign vs VUS', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('umap_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"âœ“ UMAP visualization saved: umap_visualization.png\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"âš  UMAP embedding not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš  Visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abc029-a610-4a06-bce4-5ff2da635565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 5: Fusion Model Training ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: Training Fusion Model (Cross-modal Attention + MLP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ğŸ”¥ DEBUG: í•™ìŠµ ì „ ë°ì´í„° í™•ì¸\n",
    "print(\"\\n[DEBUG] Fusion Training Data Check:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train unique values: {np.unique(y_train)}\")\n",
    "print(f\"y_train distribution: P={sum(y_train==1)}, B={sum(y_train==0)}\")\n",
    "\n",
    "model.train_fusion(\n",
    "    X_tabular=X_train,\n",
    "    node_features=node_feat,\n",
    "    adjacency_matrix=adj,\n",
    "    node_indices=node_idx_train.astype(np.int64),  # numpy arrayë¡œ í™•ì‹¤íˆ ë³€í™˜\n",
    "    y_binary=y_train.astype(np.float32),  # float32ë¡œ ë³€í™˜\n",
    "    epochs=EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# ğŸ”¥ DEBUG: í•™ìŠµ ì§í›„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n[DEBUG] Fusion Prediction Test (first 10):\")\n",
    "test_scores = model.predict(\n",
    "    X_tabular=X_train[:10],\n",
    "    node_features=node_feat,\n",
    "    adjacency_matrix=adj,\n",
    "    node_indices=node_idx_train[:10]\n",
    ")\n",
    "print(f\"Scores: {test_scores.flatten()}\")\n",
    "print(f\"Unique: {len(np.unique(test_scores))}\")\n",
    "\n",
    "# ==================== Step 6: Prediction ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: Predicting Pathogenic Scores\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPredicting all variants (including VUS)...\")\n",
    "ranking_scores = model.predict(X_all, node_feat, adj, node_idx_all)  # Raw logits (rankingìš©)\n",
    "proba_scores = model.predict_proba(X_all, node_feat, adj, node_idx_all)  # í™•ë¥  (í•´ì„ìš©)\n",
    "\n",
    "# DataFrameì— ê²°ê³¼ ì¶”ê°€\n",
    "df_all['ranking_score'] = ranking_scores  # Logits (rankingì— ì‚¬ìš©)\n",
    "df_all['pathogenic_proba'] = proba_scores  # í™•ë¥  (ì‚¬ëŒì´ ì½ê¸° ì‰¬ì›€)\n",
    "\n",
    "print(f\"âœ“ Prediction complete: {len(ranking_scores)} variants scored\")\n",
    "print(f\"  - Ranking Score (logits) range: [{ranking_scores.min():.2f}, {ranking_scores.max():.2f}]\")\n",
    "print(f\"  - Probability range: [{proba_scores.min():.4f}, {proba_scores.max():.4f}]\")\n",
    "\n",
    "# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ scores_allì€ í™•ë¥ ë¡œ ìœ ì§€\n",
    "scores_all = proba_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ef91f-9808-4e7e-a4d9-7af5f082e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 7: Score Distribution Visualization ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: Score Distribution Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Score ë¶„í¬ (Histogram)\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(scores_all[y_all == 1], bins=50, alpha=0.6, color='red', \n",
    "             label='Pathogenic (True)', edgecolor='darkred')\n",
    "    ax1.hist(scores_all[y_all == 0], bins=50, alpha=0.6, color='blue', \n",
    "             label='Benign (True)', edgecolor='darkblue')\n",
    "    ax1.hist(scores_all[y_all == -1], bins=50, alpha=0.4, color='gray', \n",
    "             label='VUS (Unknown)', edgecolor='black')\n",
    "    ax1.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "    ax1.set_xlabel('Pathogenic Score', fontsize=12)\n",
    "    ax1.set_ylabel('Count', fontsize=12)\n",
    "    ax1.set_title('Pathogenic Score Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: VUS Score ë¶„í¬ë§Œ\n",
    "    ax2 = axes[1]\n",
    "    vus_scores = scores_all[y_all == -1]\n",
    "    \n",
    "    if len(vus_scores) > 0:\n",
    "        # pd.Seriesë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•˜ê³  í¸ë¦¬í•œ í†µê³„ ê³„ì‚°\n",
    "        vus_scores = pd.Series(vus_scores, dtype='float64')\n",
    "        \n",
    "        # í†µê³„ëŸ‰ ë¯¸ë¦¬ ê³„ì‚°\n",
    "        vus_median = vus_scores.median()\n",
    "        vus_mean   = vus_scores.mean()\n",
    "        vus_std    = vus_scores.std()\n",
    "        p90        = vus_scores.quantile(0.90)\n",
    "        p95        = vus_scores.quantile(0.95)\n",
    "        \n",
    "        # Histogram\n",
    "        ax2.hist(vus_scores, bins=50, alpha=0.7, color='purple', edgecolor='darkviolet')\n",
    "        \n",
    "        # í†µê³„ì„  ê·¸ë¦¬ê¸°\n",
    "        ax2.axvline(vus_mean, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Mean: {vus_mean:.3f} (Â±{vus_std:.3f})')\n",
    "        ax2.axvline(vus_median, color='orange', linestyle='--', linewidth=2, \n",
    "                    label=f'Median: {vus_median:.3f}')\n",
    "        ax2.axvline(p90, color='darkgreen', linestyle=':', linewidth=2, \n",
    "                    label=f'P90: {p90:.3f}')\n",
    "        ax2.axvline(p95, color='darkred', linestyle=':', linewidth=2, \n",
    "                    label=f'P95: {p95:.3f}')\n",
    "        \n",
    "        # ë ˆì´ë¸” ë° íƒ€ì´í‹€\n",
    "        ax2.set_xlabel('Pathogenic Score', fontsize=12)\n",
    "        ax2.set_ylabel('Count', fontsize=12)\n",
    "        ax2.set_title(f'VUS Score Distribution (n={len(vus_scores)})', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=9, loc='upper left')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No VUS data', ha='center', va='center', \n",
    "                 fontsize=14, transform=ax2.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ“ Score distribution saved: score_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  Visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d0083-77b6-4187-bdb0-3fb19f6c02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 8: Results Analysis ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 8: Results Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# VUS í•„í„°ë§\n",
    "vus_mask = (y_all == -1)\n",
    "\n",
    "print(f\"\\n[Overall Statistics]\")\n",
    "print(f\"Total variants: {len(df_all)}\")\n",
    "print(f\"  Pathogenic: {(y_all == 1).sum()}\")\n",
    "print(f\"  Benign: {(y_all == 0).sum()}\")\n",
    "print(f\"  VUS: {vus_mask.sum()}\")\n",
    "print(f\"\\n[Seed Gene Statistics]\")\n",
    "seed_mask = df_all['is_seed_gene']\n",
    "print(f\"Total seed gene variants: {seed_mask.sum()}\")\n",
    "print(f\"  Seed + Pathogenic: {(seed_mask & (y_all == 1)).sum()}\")\n",
    "print(f\"  Seed + Benign: {(seed_mask & (y_all == 0)).sum()}\")\n",
    "print(f\"  Seed + VUS: {(vus_mask & seed_mask).sum()}\")\n",
    "\n",
    "# Score ë¶„í¬\n",
    "print(f\"\\n[Score Distribution]\")\n",
    "print(f\"Mean: {scores_all.mean():.3f}\")\n",
    "print(f\"Std: {scores_all.std():.3f}\")\n",
    "print(f\"Min: {scores_all.min():.3f}\")\n",
    "print(f\"Max: {scores_all.max():.3f}\")\n",
    "print(f\"\\n[Risk Categories]\")\n",
    "print(f\"High risk (>0.7): {(scores_all > 0.7).sum()}\")\n",
    "print(f\"Medium risk (0.4-0.7): {((scores_all >= 0.4) & (scores_all <= 0.7)).sum()}\")\n",
    "print(f\"Low risk (<0.4): {(scores_all < 0.4).sum()}\")\n",
    "\n",
    "# VUS í›„ë³´ ì¶”ì¶œ (Seed vs Novel ë¶„ë¦¬)\n",
    "print(f\"\\n[VUS Candidate Analysis]\")\n",
    "\n",
    "# Seed ìœ ì „ì ë§ˆìŠ¤í¬\n",
    "seed_gene_mask = df_all['Gene.refGene'].isin(mm_genes)\n",
    "\n",
    "# Case 1: Seed ìœ ì „ìì˜ VUS (Known genes - Clinical prioritization)\n",
    "seed_vus_mask = vus_mask & seed_gene_mask\n",
    "if seed_vus_mask.sum() > 0:\n",
    "    print(f\"\\n[Seed Gene VUS - Known MM Genes]\")\n",
    "    seed_vus = df_all[seed_vus_mask].copy()\n",
    "    seed_vus = filter_superhubs(seed_vus, verbose=True)\n",
    "    seed_vus = seed_vus.sort_values('ranking_score', ascending=False)  #Logitsë¡œ ì •ë ¬\n",
    "    \n",
    "    print(f\"Total: {len(seed_vus)} variants in {seed_vus['Gene.refGene'].nunique()} seed genes\")\n",
    "    \n",
    "    # ì¶œë ¥í•  ì»¬ëŸ¼ ì„ íƒ (ranking_scoreì™€ í™•ë¥  ëª¨ë‘ í‘œì‹œ)\n",
    "    output_cols = ['Chr', 'Start', 'End', 'Ref', 'Alt', 'Gene.refGene', \n",
    "                   'CADD_phred', 'ranking_score', 'pathogenic_proba']\n",
    "    available_cols = [col for col in output_cols if col in seed_vus.columns]\n",
    "    \n",
    "    print(f\"\\nTop 10 seed gene VUS (sorted by ranking_score):\")\n",
    "    print(seed_vus[available_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # ì €ì¥\n",
    "    seed_vus.to_csv('seed_gene_vus_ranked.csv', index=False)\n",
    "    print(f\"\\nâœ“ Seed gene VUS saved to: seed_gene_vus_ranked.csv\")\n",
    "    \n",
    "    # ê³ ìœ„í—˜ (í™•ë¥  ê¸°ì¤€)\n",
    "    high_risk_seed = seed_vus[seed_vus['pathogenic_proba'] > 0.7]\n",
    "    if len(high_risk_seed) > 0:\n",
    "        high_risk_seed.to_csv('seed_gene_vus_high_risk.csv', index=False)\n",
    "        print(f\"âœ“ High-risk seed VUS: {len(high_risk_seed)} variants (proba >0.7)\")\n",
    "else:\n",
    "    print(\"\\nâš  No seed gene VUS found\")\n",
    "\n",
    "# Case 2: Non-seed ìœ ì „ìì˜ VUS (Novel discoveries - Research prioritization)\n",
    "novel_vus_mask = vus_mask & ~seed_gene_mask\n",
    "if novel_vus_mask.sum() > 0:\n",
    "    print(f\"\\n[Novel Gene VUS - New Discoveries]\")\n",
    "    novel_vus = df_all[novel_vus_mask].copy()\n",
    "    novel_vus = filter_superhubs(novel_vus, verbose=True)  # ğŸ”¥ ì¶”ê°€!\n",
    "    novel_vus = novel_vus.sort_values('ranking_score', ascending=False)  # ğŸ”¥ Logitsë¡œ ì •ë ¬!\n",
    "    \n",
    "    print(f\"Total: {len(novel_vus)} variants in {novel_vus['Gene.refGene'].nunique()} genes\")\n",
    "    \n",
    "    # ì¶œë ¥í•  ì»¬ëŸ¼ ì„ íƒ (ranking_scoreì™€ í™•ë¥  ëª¨ë‘ í‘œì‹œ)\n",
    "    output_cols = ['Chr', 'Start', 'End', 'Ref', 'Alt', 'Gene.refGene', \n",
    "                   'CADD_phred', 'ranking_score', 'pathogenic_proba']\n",
    "    available_cols = [col for col in output_cols if col in novel_vus.columns]\n",
    "    \n",
    "    print(f\"\\nTop 20 novel candidate VUS (sorted by ranking_score):\")\n",
    "    print(novel_vus[available_cols].head(20).to_string(index=False))\n",
    "    \n",
    "    # ì €ì¥\n",
    "    novel_vus.to_csv('novel_candidate_vus.csv', index=False)\n",
    "    print(f\"\\nâœ“ Novel candidate VUS saved to: novel_candidate_vus.csv\")\n",
    "    \n",
    "    # ê³ ìœ„í—˜ (í™•ë¥  ê¸°ì¤€)\n",
    "    high_risk_novel = novel_vus[novel_vus['pathogenic_proba'] > 0.7]\n",
    "    if len(high_risk_novel) > 0:\n",
    "        high_risk_novel.to_csv('novel_candidate_vus_high_risk.csv', index=False)\n",
    "        print(f\"âœ“ High-risk novel VUS: {len(high_risk_novel)} variants (proba >0.7)\")\n",
    "else:\n",
    "    print(\"\\nâš  No novel candidate VUS found\")\n",
    "\n",
    "# ì „ì²´ ê²°ê³¼ ì €ì¥\n",
    "all_results_file = 'all_variants_with_scores.csv'\n",
    "df_all.to_csv(all_results_file, index=False)\n",
    "print(f\"\\nâœ“ All variants with scores saved to: {all_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298dcb0-bf3a-4cc5-98ce-3577e41f888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[DEBUG] Modal 1 vs Final Score Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. ë¨¼ì € 0.982014 ê·¼ì²˜ ê°’ ì°¾ê¸°\n",
    "print(\"\\n[1] Searching for 0.982 scores...\")\n",
    "close_to_target = np.isclose(df_all['pathogenic_proba'], 0.982014, atol=0.001)\n",
    "print(f\"Found {close_to_target.sum()} variants with score ~0.982\")\n",
    "\n",
    "# 2. Novel gene ë§ˆìŠ¤í¬ ì¬ìƒì„±\n",
    "seed_gene_mask = df_all['Gene.refGene'].isin(mm_genes)\n",
    "vus_mask = (y_all == -1)\n",
    "novel_vus_mask = vus_mask & ~seed_gene_mask\n",
    "\n",
    "print(f\"\\n[2] VUS breakdown:\")\n",
    "print(f\"Total VUS: {vus_mask.sum()}\")\n",
    "print(f\"Seed gene VUS: {(vus_mask & seed_gene_mask).sum()}\")\n",
    "print(f\"Novel gene VUS: {novel_vus_mask.sum()}\")\n",
    "\n",
    "# 3. Novel gene ì¤‘ 0.982 ê·¼ì²˜ì¸ ê²ƒë“¤\n",
    "novel_with_target_score = novel_vus_mask & close_to_target\n",
    "print(f\"\\n[3] Novel genes with score ~0.982: {novel_with_target_score.sum()}\")\n",
    "\n",
    "# 4. ìƒ˜í”Œ 20ê°œ í™•ì¸\n",
    "if novel_with_target_score.sum() > 0:\n",
    "    sample_indices = df_all[novel_with_target_score].index[:20]\n",
    "    \n",
    "    print(f\"\\n[4] Detailed check (first 20):\")\n",
    "    print(f\"{'Gene':<15} | {'Modal1_Prob':>12} | {'Final_Score':>12} | {'Node_Idx':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        gene = df_all.loc[idx, 'Gene.refGene']\n",
    "        \n",
    "        # Modal 1 ì¶œë ¥\n",
    "        modal1_out = model.modal1.transform(X_all[idx:idx+1])\n",
    "        modal1_prob = modal1_out[0, -1]\n",
    "        \n",
    "        # Final score\n",
    "        final_score = df_all.loc[idx, 'pathogenic_proba']\n",
    "        \n",
    "        # Node index\n",
    "        node_idx = node_idx_all[idx]\n",
    "        is_unknown = (node_idx == len(loader.gene_to_idx))\n",
    "        \n",
    "        print(f\"{gene:<15} | {modal1_prob:>12.6f} | {final_score:>12.6f} | {node_idx:>10} {'(UNKNOWN)' if is_unknown else ''}\")\n",
    "    \n",
    "    # 5. í†µê³„\n",
    "    print(f\"\\n[5] Statistics:\")\n",
    "    modal1_probs = []\n",
    "    node_indices = []\n",
    "    \n",
    "    for idx in df_all[novel_with_target_score].index:\n",
    "        modal1_out = model.modal1.transform(X_all[idx:idx+1])\n",
    "        modal1_probs.append(modal1_out[0, -1])\n",
    "        node_indices.append(node_idx_all[idx])\n",
    "    \n",
    "    modal1_probs = np.array(modal1_probs)\n",
    "    node_indices = np.array(node_indices)\n",
    "    unknown_count = (node_indices == len(loader.gene_to_idx)).sum()\n",
    "    \n",
    "    print(f\"Modal 1 Probability range: [{modal1_probs.min():.6f}, {modal1_probs.max():.6f}]\")\n",
    "    print(f\"Modal 1 Probability std: {modal1_probs.std():.6f}\")\n",
    "    print(f\"UNKNOWN nodes: {unknown_count}/{len(node_indices)} ({unknown_count/len(node_indices)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c6306-a222-4922-91fb-3e26ddff9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 9: Performance Evaluation (ë¼ë²¨ ìˆëŠ” ê²½ìš°) ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 9: Performance Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if labeled_mask.sum() > 0:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss\n",
    "    \n",
    "    y_true = y_all[labeled_mask]\n",
    "    y_pred_proba = scores_all[labeled_mask]\n",
    "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n[Classification Metrics]\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_true, y_pred_proba):.3f}\")\n",
    "    print(f\"PR-AUC: {average_precision_score(y_true, y_pred_proba):.3f}\")\n",
    "    print(f\"Brier Score: {brier_score_loss(y_true, y_pred_proba):.3f} (lower is better)\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred_binary):.3f}\")\n",
    "    \n",
    "    print(f\"\\n[Classification Report]\")\n",
    "    print(classification_report(y_true, y_pred_binary, \n",
    "                                 target_names=['Benign', 'Pathogenic']))\n",
    "    \n",
    "    print(f\"\\n[Confusion Matrix]\")\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"              B    P\")\n",
    "    print(f\"Actual  B   {cm[0,0]:4d} {cm[0,1]:4d}\")\n",
    "    print(f\"        P   {cm[1,0]:4d} {cm[1,1]:4d}\")\n",
    "    \n",
    "    # ROC Curve ì‹œê°í™”\n",
    "    try:\n",
    "        from sklearn.metrics import roc_curve\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='blue', lw=2, \n",
    "                 label=f'ROC curve (AUC = {roc_auc_score(y_true, y_pred_proba):.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\", fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ ROC curve saved: roc_curve.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"âš  ROC curve plot failed: {e}\")\n",
    "else:\n",
    "    print(\"âš  No labeled data for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d62e6-8fc0-4bac-99cb-a4ee20a206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve ì‹œê°í™” (ê³„ë‹¨ ì œê±°: PCHIPìœ¼ë¡œ 'í”Œë¡¯ë§Œ' ìŠ¤ë¬´ë”©)\n",
    "try:\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    import numpy as np\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)  # AUCëŠ” ì› ì ë“¤ë¡œ ê³„ì‚°(ì •ì§)\n",
    "\n",
    "    # ë‹¨ì¡° ë³´ì¡´ ìŠ¤í”Œë¼ì¸ ë³´ê°„(ê³„ë‹¨ ìµœì†Œí™”). fpr ì¤‘ë³µ ì œê±° í›„ ë³´ê°„.\n",
    "    from scipy.interpolate import PchipInterpolator\n",
    "    ufpr, idx = np.unique(fpr, return_index=True)\n",
    "    utpr = tpr[idx]\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    pchip = PchipInterpolator(ufpr, utpr)\n",
    "    y = np.clip(pchip(x), 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6), dpi=180)\n",
    "    # ë¶€ë“œëŸ¬ìš´ ê³¡ì„ \n",
    "    plt.plot(x, y, linewidth=3, label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "    # ì›ë˜ ê³„ë‹¨ ì ë“¤ë„ í¬ë¯¸í•˜ê²Œ(ì„ íƒ: ì•ˆ ì˜ˆì˜ë©´ ì§€ì›Œë„ ë¨)\n",
    "    plt.step(fpr, tpr, where=\"post\", linewidth=1, alpha=0.25)\n",
    "\n",
    "    # ë¬´ì‘ìœ„ ê¸°ì¤€ì„ \n",
    "    plt.plot([0, 1], [0, 1], \"--\", linewidth=1.2, label=\"Random\")\n",
    "\n",
    "    # ë³´ê¸° ì¢‹ê²Œ ë‹¤ë“¬ê¸°\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(0, 1); ax.set_ylim(0, 1.02)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.grid(True, linestyle=\":\", linewidth=0.8, alpha=0.6)\n",
    "    for s in (\"top\", \"right\"): ax.spines[s].set_visible(False)\n",
    "\n",
    "    plt.title(\"ROC Curve\", fontsize=18, weight=\"bold\")\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=13)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=13)\n",
    "    plt.tick_params(axis=\"both\", labelsize=11)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, framealpha=0.9, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ë²¡í„°+ë¹„íŠ¸ë§µ ë‘˜ ë‹¤ ì €ì¥\n",
    "    plt.savefig(\"roc_curve.svg\")                     # ë…¼ë¬¸ìš©(ë²¡í„°)\n",
    "    plt.savefig(\"roc_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"âœ“ ROC curve saved: roc_curve.svg, roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš  ROC curve plot failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3404fb-0aca-4e27-ad2b-69a843408d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Final Summary ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n[Generated Files]\")\n",
    "print(f\"âœ“ umap_visualization.png - UMAP embedding space\")\n",
    "print(f\"âœ“ score_distribution.png - Score distributions\")\n",
    "print(f\"âœ“ roc_curve.png - ROC curve (if labeled data available)\")\n",
    "print(f\"âœ“ all_variants_with_scores.csv - All variants with predictions\")\n",
    "print(f\"\\n[VUS Candidates - Separated by Type]\")\n",
    "if os.path.exists('seed_gene_vus_ranked.csv'):\n",
    "    print(f\"âœ“ seed_gene_vus_ranked.csv - Known MM genes VUS (clinical use)\")\n",
    "if os.path.exists('seed_gene_vus_high_risk.csv'):\n",
    "    print(f\"âœ“ seed_gene_vus_high_risk.csv - High-risk seed VUS (>0.7)\")\n",
    "if os.path.exists('novel_candidate_vus.csv'):\n",
    "    print(f\"âœ“ novel_candidate_vus.csv - Novel gene candidates (research use)\")\n",
    "if os.path.exists('novel_candidate_vus_high_risk.csv'):\n",
    "    print(f\"âœ“ novel_candidate_vus_high_risk.csv - High-risk novel VUS (>0.7)\")\n",
    "\n",
    "print(f\"\\n[Next Steps]\")\n",
    "print(f\"1. Review UMAP visualization for data quality\")\n",
    "print(f\"2. Seed gene VUS: Use for clinical decision-making\")\n",
    "print(f\"3. Novel candidate VUS: Investigate for new MM associations\")\n",
    "print(f\"4. Validate top predictions with literature/databases\")\n",
    "print(f\"5. Consider functional validation of high-risk novel candidates\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Thank you for using MM Pathogenic Variant Prediction!\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27602ed-330f-49d3-b0d2-d4b5e53206ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5828146-0cd3-4ad3-9fd4-a4c69e715b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_clnsig(x: str) -> str:\n",
    "    s = (str(x) or \"\").strip()\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return s if s != \".\" else \".\"\n",
    "\n",
    "def is_pathogenic_like(s: str) -> bool:\n",
    "    s = normalize_clnsig(s)\n",
    "    return s in {\n",
    "        \"Pathogenic\",\"Likely_pathogenic\",\"Pathogenic/Likely_pathogenic\"\n",
    "    }\n",
    "\n",
    "def is_benign_like(s: str) -> bool:\n",
    "    s = normalize_clnsig(s)\n",
    "    return s in {\n",
    "        \"Benign\",\"Likely_benign\",\"Benign/Likely_benign\"\n",
    "    }\n",
    "\n",
    "def is_vus_like(s: str) -> bool:\n",
    "    s = normalize_clnsig(s)\n",
    "    # VUSë¡œ ì·¨ê¸‰: '.', Uncertain*, Conflicting*, not_provided, risk_factor ë“±\n",
    "    return (\n",
    "        s == \".\" or\n",
    "        s.startswith(\"Uncertain\") or\n",
    "        s.startswith(\"Conflicting\") or\n",
    "        s in {\"not_provided\",\"risk_factor\",\"no_classification_for_the_single_variant\"}\n",
    "    )\n",
    "\n",
    "# df_all: ì ìˆ˜ ë¶™ì€ ìµœì¢… ë°ì´í„°í”„ë ˆì„ ê°€ì •\n",
    "df_all[\"CLNSIG_norm\"] = df_all[\"CLNSIG\"].map(normalize_clnsig)\n",
    "\n",
    "# Top 20 ì„ ë³„ ë¡œì§(í˜„ì¬ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•˜ê²Œ ì •ë ¬)\n",
    "top20 = df_all.sort_values(\"pathogenic_proba\", ascending=False).head(20).copy()\n",
    "\n",
    "# Top20 ì•ˆì— ë¼ë²¨ ì„ì„ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "leak_mask = top20[\"CLNSIG_norm\"].apply(lambda s: is_pathogenic_like(s) or is_benign_like(s))\n",
    "n_leak = int(leak_mask.sum())\n",
    "\n",
    "print(\"\\n[Leakage Check on Top 20]\")\n",
    "print(f\"  Labeled (Pathogenic/Benign-like) in Top20: {n_leak}\")\n",
    "if n_leak > 0:\n",
    "    print(\"  âš  Found labeled items inside Top-20 candidates. Showing them:\")\n",
    "    display_cols = [\"Chr\",\"Start\",\"End\",\"Ref\",\"Alt\",\"Gene.refGene\",\"CLNSIG\",\"pathogenic_proba\"]\n",
    "    print(top20.loc[leak_mask, display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"  âœ“ No labeled items inside Top-20.\")\n",
    "\n",
    "# â€˜ì§„ì§œ VUS í›„ë³´ Top20â€™ ì¬ê³„ì‚°(ë¼ë²¨ ì—„ê²© ì œì™¸)\n",
    "mask_vus = df_all[\"CLNSIG_norm\"].apply(is_vus_like)\n",
    "top20_vus = (\n",
    "    df_all.loc[mask_vus]\n",
    "    .sort_values(\"pathogenic_proba\", ascending=False)\n",
    "    .head(20)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "print(\"\\n[Top 20 â€” VUS-only]\")\n",
    "display_cols = [\"Chr\",\"Start\",\"End\",\"Ref\",\"Alt\",\"Gene.refGene\",\"CLNSIG\",\"pathogenic_proba\"]\n",
    "print(top20_vus[display_cols].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc05d65-2cfe-4beb-ae49-c8d68bfd4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì „ìë³„ ìµœê³  ìŠ¤ì½”ì–´\n",
    "gene_top = (\n",
    "    df_all\n",
    "    .groupby(\"Gene.refGene\", as_index=False)\n",
    "    .agg(\n",
    "        best_score=(\"pathogenic_proba\", \"max\"),\n",
    "        n_variants=(\"pathogenic_proba\", \"size\")\n",
    "    )\n",
    "    .sort_values(\"best_score\", ascending=False)\n",
    ")\n",
    "\n",
    "gene_top.head(50).to_csv(\"gene_level_vus_ranking.csv\", index=False)\n",
    "gene_top.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989be03-1c52-4609-a03d-d4c9d4535c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e406cc3-b9a0-471d-a0cb-ca9fd279cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Modal1 ìª½ì—ì„œ \"ì •ê·œí™” + í™•ë¥ \"ê¹Œì§€ í¬í•¨ëœ 20D í”¼ì²˜ ë½‘ê¸°\n",
    "modal1_features = model.modal1.transform(X_all)\n",
    "modal1_prob = modal1_features[:, -1]   # ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ P(Pathogenic)\n",
    "\n",
    "# 2) ìµœì¢… ì ìˆ˜ (fusion + GCNê¹Œì§€ ì§€ë‚œ ì ìˆ˜)\n",
    "final_score = scores_all  # ë˜ëŠ” df_all[\"pathogenic_score\"].values\n",
    "\n",
    "# 3) ìƒê´€ê³„ìˆ˜ + ì°¨ì´ ë¶„í¬ ë³´ê¸°\n",
    "corr = np.corrcoef(modal1_prob, final_score)[0, 1]\n",
    "print(\"corr(modal1, final) =\", corr)\n",
    "\n",
    "diff = np.abs(modal1_prob - final_score)\n",
    "print(\"abs diff summary:\")\n",
    "print(pd.Series(diff).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c82a3-b147-45b2-b42d-67692a42d53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c727e0-a21f-4db8-9a29-06ddf23e9892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9138c567-ffaa-40e2-8372-bc366980a7cb",
   "metadata": {},
   "source": [
    "# IF you have Pretrained-Pkl file. Try  it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e542560-59bf-4012-98e6-5fab5e520862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) ëª¨ë¸ & ë„¤íŠ¸ì›Œí¬ ë¡œë”©\n",
    "with open('trained_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('network_data.pkl', 'rb') as f:\n",
    "    network_data = pickle.load(f)\n",
    "\n",
    "ppi_graph = network_data['ppi_graph']\n",
    "node_features = network_data['node_features']\n",
    "adjacency_matrix = network_data['adjacency_matrix']\n",
    "\n",
    "# 2) í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©\n",
    "test_file = annovar_file\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# í•„ìš”í•˜ë©´ exonic/splicing í•„í„°, synonymous ì œê±°ë„ ì—¬ê¸°ì„œ ì ê¹ í•´ë„ ë¨\n",
    "\n",
    "# 3) feature & gene ë§¤í•‘\n",
    "loader = VariantDataLoader()\n",
    "\n",
    "X_test, feat_names = loader.preprocess_features(df_test)\n",
    "gene_list = loader.extract_gene_mapping(df_test, gene_column='Gene.refGene')\n",
    "node_indices = loader.create_gene_to_node_mapping(gene_list, ppi_graph)\n",
    "\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"node_indices:\", node_indices.shape)\n",
    "\n",
    "# 4) ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
    "y_pred_proba = model.predict_proba(\n",
    "    X_tabular=X_test,\n",
    "    node_features=node_features,\n",
    "    adjacency_matrix=adjacency_matrix,\n",
    "    node_indices=node_indices\n",
    ")\n",
    "\n",
    "print(\"ì˜ˆì¸¡ í™•ë¥  ì˜ˆì‹œ (ì• 5ê°œ):\", y_pred_proba[:5])\n",
    "print(\"í‰ê· :\", np.mean(y_pred_proba), \"ìµœëŒ€:\", np.max(y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5919f69-a224-477b-a88f-85398a022e26",
   "metadata": {},
   "source": [
    "## Funtion for pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf111e1e-84d8-4ec3-b72f-f7355eae0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def filter_superhubs(df, gene_column='Gene.refGene', verbose=True):\n",
    "    SUPER_HUB_GENES = [\n",
    "        'UBC', 'UBB', 'UBA52', 'RPS27A',\n",
    "        'HSP90AA1', 'HSP90AB1', 'HSPA8', 'HSPA1A', 'HSPA1B',\n",
    "        'HSPA5', 'HSPA9', 'HSP90B1', 'HSPD1', 'HSPE1',\n",
    "        'ACTB', 'ACTG1', 'TUBA1A', 'TUBA1B', 'TUBB',\n",
    "    ]\n",
    "    mask_explicit = df[gene_column].isin(SUPER_HUB_GENES)\n",
    "    mask_hist = df[gene_column].str.startswith('HIST', na=False)\n",
    "    mask_rpl = df[gene_column].str.startswith('RPL', na=False)\n",
    "    mask_rps = df[gene_column].str.startswith('RPS', na=False)\n",
    "    superhub_mask = mask_explicit | mask_hist | mask_rpl | mask_rps\n",
    "    filtered_df = df[~superhub_mask].copy()\n",
    "    removed_count = superhub_mask.sum()\n",
    "    if verbose and removed_count > 0:\n",
    "        print(f\"  ğŸ”¥ Filtered out {removed_count} super-hub variants\")\n",
    "        removed_genes = df[superhub_mask][gene_column].value_counts().head(5)\n",
    "        if len(removed_genes) > 0:\n",
    "            print(f\"     Top removed genes: {', '.join(removed_genes.index.tolist())}\")\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def predict_new_variants(test_file, \n",
    "                         model_file='trained_model.pkl',\n",
    "                         network_file='network_data.pkl',\n",
    "                         output_dir='test_results',\n",
    "                         verbose=True):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"MM Pathogenic Variant Prediction - TEST MODE\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: ëª¨ë¸ & ë„¤íŠ¸ì›Œí¬ ë¡œë“œ\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 1: Loading Trained Model & Network Data\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    if not os.path.exists(model_file):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_file}\")\n",
    "    if not os.path.exists(network_file):\n",
    "        raise FileNotFoundError(f\"Network file not found: {network_file}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loading model from: {model_file}\")\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    if verbose:\n",
    "        print(\"âœ“ Model loaded\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nLoading network data from: {network_file}\")\n",
    "    with open(network_file, 'rb') as f:\n",
    "        network_data = pickle.load(f)\n",
    "    \n",
    "    ppi_graph = network_data['ppi_graph']\n",
    "    mm_genes = network_data['mm_genes']\n",
    "    node_feat = network_data['node_features']\n",
    "    adj = network_data['adjacency_matrix']\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"âœ“ Network loaded:\")\n",
    "        print(f\"  - PPI nodes: {ppi_graph.number_of_nodes()}\")\n",
    "        print(f\"  - PPI edges: {ppi_graph.number_of_edges()}\")\n",
    "        print(f\"  - MM seed genes: {len(mm_genes)}\")\n",
    "        print(f\"  - Node features: {node_feat.shape}\")\n",
    "    \n",
    "    # STEP 2: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 2: Loading Test Data\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Test file: {test_file}\")\n",
    "    \n",
    "    loader = VariantDataLoader()\n",
    "    \n",
    "    # ì—¬ê¸°! load_annovar_data ì‚¬ìš©\n",
    "    df_test = loader.load_annovar_data(test_file)\n",
    "    if verbose:\n",
    "        print(f\"âœ“ Loaded {len(df_test)} variants\")\n",
    "    \n",
    "    # Exonic / Splicing í•„í„°\n",
    "    if 'Func.refGene' in df_test.columns:\n",
    "        func_mask = (\n",
    "            df_test['Func.refGene'].str.contains('exonic', case=False, na=False) |\n",
    "            df_test['Func.refGene'].str.contains('splicing', case=False, na=False)\n",
    "        )\n",
    "        df_test = df_test[func_mask].copy()\n",
    "        if verbose:\n",
    "            print(f\"âœ“ Filtered to exonic/splicing: {len(df_test)}\")\n",
    "    \n",
    "    # Synonymous SNV ì œì™¸\n",
    "    if 'ExonicFunc.refGene' in df_test.columns:\n",
    "        syn_mask = df_test['ExonicFunc.refGene'].str.contains('synonymous SNV', case=False, na=False)\n",
    "        df_test = df_test[~syn_mask].copy()\n",
    "        if verbose:\n",
    "            print(f\"âœ“ Removed synonymous SNV: {len(df_test)}\")\n",
    "    \n",
    "    # 19ê°œ íŠ¹ì„± ì¶”ì¶œ\n",
    "    X_test, feature_names = loader.preprocess_features(df_test)\n",
    "    if verbose:\n",
    "        print(f\"âœ“ Extracted features: {X_test.shape}\")\n",
    "    \n",
    "    # Gene â†’ Node ë§¤í•‘\n",
    "    gene_list = loader.extract_gene_mapping(df_test, gene_column='Gene.refGene')\n",
    "    node_idx_test = loader.create_gene_to_node_mapping(gene_list, ppi_graph)\n",
    "    if verbose:\n",
    "        print(f\"âœ“ Mapped to network nodes (unique={len(set(node_idx_test))})\")\n",
    "    \n",
    "    # STEP 3: ì˜ˆì¸¡\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 3: Making Predictions\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Running model predictions...\")\n",
    "    \n",
    "    ranking_scores = model.predict(X_test, node_feat, adj, node_idx_test)\n",
    "    proba_scores   = model.predict_proba(X_test, node_feat, adj, node_idx_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"âœ“ Predictions complete: {len(ranking_scores)} variants\")\n",
    "        print(f\"  - Ranking Score range: [{ranking_scores.min():.2f}, {ranking_scores.max():.2f}]\")\n",
    "        print(f\"  - Probability range: [{proba_scores.min():.4f}, {proba_scores.max():.4f}]\")\n",
    "    \n",
    "    df_test['ranking_score'] = ranking_scores\n",
    "    df_test['pathogenic_proba'] = proba_scores\n",
    "    \n",
    "    # â€¦ ì´í•˜ STEP 4~6 (seed/novel ë‚˜ëˆ„ê³ , superhub í•„í„°, csv ì €ì¥, ê·¸ë¦¼ ê·¸ë¦¬ëŠ” ë¶€ë¶„)\n",
    "    # ì€ ë„¤ê°€ ì˜¬ë¦° ì½”ë“œ ê·¸ëŒ€ë¡œ ì¨ë„ ë¨ (ê·¸ ë¶€ë¶„ì€ ì¸í„°í˜ì´ìŠ¤ ì•ˆ ê¹¨ì§)\n",
    "\n",
    "    # ë§ˆì§€ë§‰ì— stats, results dict ë§Œë“œëŠ” ë¶€ë¶„ë„ ê·¸ëŒ€ë¡œ ìœ ì§€ ê°€ëŠ¥\n",
    "    # (ì›ë˜ ì½”ë“œ ë³µë¶™í•´ì„œ ì“°ë©´ ë¨)\n",
    "\n",
    "    # ==================== Step 4: Results Analysis ====================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 4: Results Analysis\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # í†µê³„\n",
    "    stats = {\n",
    "        'total_variants': len(df_test),\n",
    "        'ranking_score_mean': ranking_scores.mean(),\n",
    "        'ranking_score_std': ranking_scores.std(),\n",
    "        'proba_mean': proba_scores.mean(),\n",
    "        'proba_std': proba_scores.std(),\n",
    "        'high_risk_count': (proba_scores > 0.7).sum(),\n",
    "        'medium_risk_count': ((proba_scores >= 0.4) & (proba_scores <= 0.7)).sum(),\n",
    "        'low_risk_count': (proba_scores < 0.4).sum(),\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n[Score Distribution]\")\n",
    "        print(f\"Ranking Score - Mean: {stats['ranking_score_mean']:.3f}, Std: {stats['ranking_score_std']:.3f}\")\n",
    "        print(f\"Probability   - Mean: {stats['proba_mean']:.3f}, Std: {stats['proba_std']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n[Risk Categories by Probability]\")\n",
    "        print(f\"High risk (>0.7):      {stats['high_risk_count']} variants ({stats['high_risk_count']/len(proba_scores)*100:.1f}%)\")\n",
    "        print(f\"Medium risk (0.4-0.7): {stats['medium_risk_count']} variants\")\n",
    "        print(f\"Low risk (<0.4):       {stats['low_risk_count']} variants\")\n",
    "    \n",
    "    # Seed vs Novel ë¶„ë¦¬\n",
    "    seed_mask = df_test['Gene.refGene'].isin(mm_genes)\n",
    "    stats['seed_count'] = seed_mask.sum()\n",
    "    stats['novel_count'] = (~seed_mask).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[Seed vs Novel Genes]\")\n",
    "        print(f\"Seed gene variants:  {stats['seed_count']} ({stats['seed_count']/len(df_test)*100:.1f}%)\")\n",
    "        print(f\"Novel gene variants: {stats['novel_count']} ({stats['novel_count']/len(df_test)*100:.1f}%)\")\n",
    "    \n",
    "    # ==================== Step 5: Top Candidates ====================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 5: Top Candidate Variants\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Seed gene variants\n",
    "    if seed_mask.sum() > 0:\n",
    "        if verbose:\n",
    "            print(f\"\\n[Top 10 Seed Gene Variants]\")\n",
    "        seed_variants = df_test[seed_mask].copy()\n",
    "        seed_variants = filter_superhubs(seed_variants, verbose=verbose)\n",
    "        seed_variants = seed_variants.sort_values('ranking_score', ascending=False)\n",
    "        \n",
    "        output_cols = ['Chr', 'Start', 'End', 'Ref', 'Alt', 'Gene.refGene', \n",
    "                       'CADD_phred', 'CLNSIG', 'ranking_score', 'pathogenic_proba', 'AF_patient']\n",
    "        available_cols = [col for col in output_cols if col in seed_variants.columns]\n",
    "        \n",
    "        if verbose:\n",
    "            print(seed_variants[available_cols].head(10).to_string(index=False))\n",
    "        \n",
    "        # ì €ì¥\n",
    "        seed_file = os.path.join(output_dir, 'seed_gene_predictions.csv')\n",
    "        seed_variants.to_csv(seed_file, index=False)\n",
    "        if verbose:\n",
    "            print(f\"\\nâœ“ Saved to: {seed_file}\")\n",
    "        \n",
    "        results['seed_variants'] = seed_variants\n",
    "    else:\n",
    "        results['seed_variants'] = None\n",
    "    \n",
    "    # Novel gene variants\n",
    "    if (~seed_mask).sum() > 0:\n",
    "        if verbose:\n",
    "            print(f\"\\n[Top 20 Novel Gene Variants]\")\n",
    "        novel_variants = df_test[~seed_mask].copy()\n",
    "        novel_variants = filter_superhubs(novel_variants, verbose=verbose)\n",
    "        novel_variants = novel_variants.sort_values('ranking_score', ascending=False)\n",
    "        \n",
    "        output_cols = ['Chr', 'Start', 'End', 'Ref', 'Alt', 'Gene.refGene', \n",
    "                       'CADD_phred', 'CLNSIG', 'ranking_score', 'pathogenic_proba','AF_patient']\n",
    "        available_cols = [col for col in output_cols if col in novel_variants.columns]\n",
    "        \n",
    "        if verbose:\n",
    "            print(novel_variants[available_cols].head(20).to_string(index=False))\n",
    "        \n",
    "        # ì €ì¥\n",
    "        novel_file = os.path.join(output_dir, 'novel_gene_predictions.csv')\n",
    "        novel_variants.to_csv(novel_file, index=False)\n",
    "        if verbose:\n",
    "            print(f\"\\nâœ“ Saved to: {novel_file}\")\n",
    "        \n",
    "        results['novel_variants'] = novel_variants\n",
    "    else:\n",
    "        results['novel_variants'] = None\n",
    "    \n",
    "    # ì „ì²´ ê²°ê³¼ ì €ì¥\n",
    "    all_file = os.path.join(output_dir, 'all_predictions.csv')\n",
    "    df_test_sorted = df_test.sort_values('ranking_score', ascending=False)\n",
    "    df_test_sorted.to_csv(all_file, index=False)\n",
    "    if verbose:\n",
    "        print(f\"\\nâœ“ All predictions saved to: {all_file}\")\n",
    "    \n",
    "    results['df_all'] = df_test_sorted\n",
    "    \n",
    "    # High-risk variants\n",
    "    high_risk = df_test[df_test['pathogenic_proba'] > 0.7].copy()\n",
    "    if len(high_risk) > 0:\n",
    "        high_risk = filter_superhubs(high_risk, verbose=False)\n",
    "        high_risk = high_risk.sort_values('ranking_score', ascending=False)\n",
    "        high_risk_file = os.path.join(output_dir, 'high_risk_predictions.csv')\n",
    "        high_risk.to_csv(high_risk_file, index=False)\n",
    "        if verbose:\n",
    "            print(f\"âœ“ High-risk variants ({len(high_risk)}): {high_risk_file}\")\n",
    "        results['high_risk'] = high_risk\n",
    "    else:\n",
    "        results['high_risk'] = None\n",
    "    \n",
    "    # ==================== Step 6: Visualization ====================\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 6: Creating Visualizations\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Plot 1: Score distribution\n",
    "        ax1 = axes[0]\n",
    "        ax1.hist(proba_scores, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "        ax1.axvline(proba_scores.mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {proba_scores.mean():.3f}')\n",
    "        ax1.axvline(0.7, color='orange', linestyle=':', linewidth=2,\n",
    "                    label='High-risk threshold (0.7)')\n",
    "        ax1.set_xlabel('Pathogenic Probability', fontsize=12)\n",
    "        ax1.set_ylabel('Count', fontsize=12)\n",
    "        ax1.set_title('Test Set - Score Distribution', fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Seed vs Novel\n",
    "        ax2 = axes[1]\n",
    "        seed_scores = proba_scores[seed_mask]\n",
    "        novel_scores = proba_scores[~seed_mask]\n",
    "        \n",
    "        if len(seed_scores) > 0:\n",
    "            ax2.hist(seed_scores, bins=30, alpha=0.6, color='blue', \n",
    "                     label=f'Seed genes (n={len(seed_scores)})', edgecolor='darkblue')\n",
    "        if len(novel_scores) > 0:\n",
    "            ax2.hist(novel_scores, bins=30, alpha=0.6, color='green',\n",
    "                     label=f'Novel genes (n={len(novel_scores)})', edgecolor='darkgreen')\n",
    "        \n",
    "        ax2.set_xlabel('Pathogenic Probability', fontsize=12)\n",
    "        ax2.set_ylabel('Count', fontsize=12)\n",
    "        ax2.set_title('Seed vs Novel Genes', fontsize=14, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_file = os.path.join(output_dir, 'test_predictions_distribution.png')\n",
    "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "        if verbose:\n",
    "            print(f\"âœ“ Visualization saved: {plot_file}\")\n",
    "        plt.show()  # Jupyterì—ì„œ ë°”ë¡œ í‘œì‹œ\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âš  Visualization failed: {e}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TEST PREDICTION COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nResults saved to: {output_dir}/\")\n",
    "        print(f\"  - all_predictions.csv\")\n",
    "        if results['seed_variants'] is not None:\n",
    "            print(f\"  - seed_gene_predictions.csv\")\n",
    "        if results['novel_variants'] is not None:\n",
    "            print(f\"  - novel_gene_predictions.csv\")\n",
    "        if results['high_risk'] is not None:\n",
    "            print(f\"  - high_risk_predictions.csv\")\n",
    "        print(f\"  - test_predictions_distribution.png\")\n",
    "    \n",
    "    results['stats'] = stats\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866d296-9051-4c26-9554-5f451e7a6c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pretained-test #if have\n",
    "test_file = r\"clnvr140401_test_code.csv\" #if have\n",
    "\n",
    "predict_new_variants(test_file, \n",
    "                         model_file='trained_model.pkl', #if have\n",
    "                         network_file='network_data.pkl', #if have\n",
    "                         output_dir='test_results',\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fe293-1146-44a6-823f-22758235943b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
